```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Define the custom CUDA kernel for fused tanh, scaling, and bias addition
source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_tanh_scale_bias_kernel(const float* __restrict__ in_data,
                                             const float* __restrict__ bias_data,
                                             const float scaling_factor,
                                             float* __restrict__ out_data,
                                             const int batch_size,
                                             const int channels,
                                             const int height,
                                             const int width) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total_size = batch_size * channels * height * width;
    if (idx < total_size) {
        int w = idx % width;
        int h = (idx / width) % height;
        int c = (idx / (width * height)) % channels;
        // N is idx / (channels*height*width), but not needed explicitly here except for clarity
        // int n = idx / (channels * height * width);

        float val = in_data[idx];
        // tanh, scale, and add bias
        float out_val = tanhf(val) * scaling_factor + bias_data[c];
        out_data[idx] = out_val;
    }
}

torch::Tensor fused_tanh_scale_bias_cuda(torch::Tensor x,
                                         torch::Tensor bias,
                                         float scaling_factor) {
    // x is expected to be NCHW
    // bias has shape (channels, 1, 1), but we'll only read bias by channel index
    const auto batch_size = x.size(0);
    const auto channels   = x.size(1);
    const auto height     = x.size(2);
    const auto width      = x.size(3);

    auto out = torch::empty_like(x);

    int total_size = batch_size * channels * height * width;
    const int block = 256;
    const int grid = (total_size + block - 1) / block;

    fused_tanh_scale_bias_kernel<<<grid, block>>>(
        x.data_ptr<float>(),
        bias.data_ptr<float>(),
        scaling_factor,
        out.data_ptr<float>(),
        batch_size,
        channels,
        height,
        width
    );

    return out;
}
"""

cpp_src = r"""
torch::Tensor fused_tanh_scale_bias_cuda(torch::Tensor x,
                                         torch::Tensor bias,
                                         float scaling_factor);
"""

# Compile the inline CUDA code for fused tanh + scale + bias
fused_tanh_scale_bias = load_inline(
    name="fused_tanh_scale_bias",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=["fused_tanh_scale_bias_cuda"],
    verbose=False,
    extra_cflags=["-O2"],
    extra_ldflags=[],
)

class ModelNew(nn.Module):
    """
    A model that performs a convolution, then uses a fused custom CUDA kernel to do tanh, scaling, and bias addition,
    followed by a max-pool.
    """
    def __init__(self, in_channels, out_channels, kernel_size, scaling_factor, bias_shape, pool_kernel_size):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.scaling_factor = scaling_factor
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.max_pool = nn.MaxPool2d(pool_kernel_size)

        # Expose our fused operator
        self.fused_tanh_scale_bias = fused_tanh_scale_bias

    def forward(self, x):
        # Convolution
        x = self.conv(x)
        # Fused tanh, scaling, and bias addition
        x = self.fused_tanh_scale_bias.fused_tanh_scale_bias_cuda(x, self.bias, self.scaling_factor)
        # Max-pooling
        x = self.max_pool(x)
        return x
```