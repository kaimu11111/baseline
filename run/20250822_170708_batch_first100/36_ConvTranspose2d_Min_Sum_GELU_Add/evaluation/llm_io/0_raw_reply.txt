```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Hand-written CUDA kernels for channel-wise min, sum over height, and GELU
source = r'''
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>

__device__ __forceinline__ float gelu_approx(float x) {
    // An approximate GELU
    const float kAlpha = 0.7978845608f;  // sqrt(2 / pi)
    return 0.5f * x * (1.0f + tanhf(kAlpha * (x + 0.044715f * x * x * x)));
}

__global__ void channel_min_kernel(const float* __restrict__ input,
                                   float* __restrict__ output,
                                   int N, int C, int H, int W) {
    // We produce shape [N, 1, H, W]. One value per (N, H, W).
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total = N * H * W;
    if (idx < total) {
        int w = idx % W;
        int h = (idx / W) % H;
        int n = idx / (H * W);
        float min_val = 3.402823466e+38F; // FLT_MAX
        for (int c = 0; c < C; c++) {
            float val = input[n*C*H*W + c*H*W + h*W + w];
            if (val < min_val) {
                min_val = val;
            }
        }
        output[n*H*W + h*W + w] = min_val;
    }
}

__global__ void sum_height_kernel(const float* __restrict__ input,
                                  float* __restrict__ output,
                                  int N, int H, int W) {
    // Input shape is [N, 1, H, W], output shape is [N, 1, 1, W].
    // We'll sum along the H dimension for each (N, W).
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total = N * W;
    if (idx < total) {
        int w = idx % W;
        int n = idx / W;
        float s = 0.0f;
        for (int h = 0; h < H; h++) {
            s += input[n*H*W + h*W + w];
        }
        output[n*W + w] = s;
    }
}

__global__ void gelu_inplace_kernel(float* __restrict__ data, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        data[idx] = gelu_approx(data[idx]);
    }
}

torch::Tensor channel_min_cuda(torch::Tensor input) {
    // input.shape: [N, C, H, W]
    auto sizes = input.sizes();
    int N = sizes[0];
    int C = sizes[1];
    int H = sizes[2];
    int W = sizes[3];

    // Output shape [N, 1, H, W]
    auto out = torch::zeros({N, 1, H, W}, input.options());

    int blockSize = 256;
    int gridSize = (N * H * W + blockSize - 1) / blockSize;

    channel_min_kernel<<<gridSize, blockSize>>>(
        input.data_ptr<float>(),
        out.data_ptr<float>(),
        N, C, H, W
    );
    return out;
}

torch::Tensor sum_height_cuda(torch::Tensor input) {
    // input.shape: [N, 1, H, W]
    auto sizes = input.sizes();
    int N = sizes[0];
    int H = sizes[2];
    int W = sizes[3];

    // Output shape [N, 1, 1, W]
    auto out = torch::zeros({N, 1, 1, W}, input.options());

    int blockSize = 256;
    int gridSize = (N * W + blockSize - 1) / blockSize;

    sum_height_kernel<<<gridSize, blockSize>>>(
        input.data_ptr<float>(),
        out.data_ptr<float>(),
        N, H, W
    );
    return out;
}

torch::Tensor gelu_inplace_cuda(torch::Tensor input) {
    // Apply GELU in-place. Input is e.g. [N, 1, 1, W].
    auto size = input.numel();
    int blockSize = 256;
    int gridSize = (size + blockSize - 1) / blockSize;

    gelu_inplace_kernel<<<gridSize, blockSize>>>(
        input.data_ptr<float>(),
        size
    );
    return input;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("channel_min_cuda", &channel_min_cuda, "Channel-Wise Min (CUDA)");
    m.def("sum_height_cuda", &sum_height_cuda, "Sum Over Height (CUDA)");
    m.def("gelu_inplace_cuda", &gelu_inplace_cuda, "Approx GELU In-Place (CUDA)");
}
'''

cpp_src = r'''
torch::Tensor channel_min_cuda(torch::Tensor input);
torch::Tensor sum_height_cuda(torch::Tensor input);
torch::Tensor gelu_inplace_cuda(torch::Tensor input);
'''

# Load/compile our custom CUDA module
custom_ops = load_inline(
    name="custom_fused_ops",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=["channel_min_cuda", "sum_height_cuda", "gelu_inplace_cuda"],
    verbose=False,
    extra_cflags=[],
    extra_ldflags=[]
)

class ModelNew(nn.Module):
    """
    Optimized model: uses custom CUDA kernels for min, sum, and GELU.
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding)
        self.bias = nn.Parameter(torch.randn(bias_shape))

    def forward(self, x):
        # Standard transpose convolution in PyTorch
        x = self.conv_transpose(x)
        # Custom CUDA kernel for min over channel dimension
        x = custom_ops.channel_min_cuda(x)
        # Custom CUDA kernel for sum over the height dimension
        x = custom_ops.sum_height_cuda(x)
        # Apply in-place approximate GELU via custom CUDA kernel
        x = custom_ops.gelu_inplace_cuda(x)
        # Final addition of bias
        x = x + self.bias
        return x
```
