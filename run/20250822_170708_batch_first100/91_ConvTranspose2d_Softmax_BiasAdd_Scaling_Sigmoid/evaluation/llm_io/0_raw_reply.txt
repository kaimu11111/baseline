```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Fused custom CUDA kernel that performs:
#  1) channelwise softmax over dim=1
#  2) adds bias
#  3) multiplies by scaling factor
#  4) applies sigmoid
# all in one pass over each (N,H,W), looping over channels inside the kernel.

fused_source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

// Kernel to fuse softmax (dim=1), add bias, scale, and sigmoid.
__global__ void fused_softmax_bias_scale_sigmoid_kernel(
    const float* __restrict__ input,
    const float* __restrict__ bias,
    float* __restrict__ output,
    const int N,
    const int C,
    const int H,
    const int W,
    const float scaling
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int totalThreads = N * H * W;
    if (idx >= totalThreads) return;

    // Decompose idx into (n, h, w)
    int n = idx / (H * W);
    int r = idx % (H * W);
    int h = r / W;
    int w = r % W;

    // 1) Find max over channels
    float max_val = -1e30f;
    for(int c = 0; c < C; c++){
        float val = input[n*C*H*W + c*H*W + h*W + w];
        max_val = (val > max_val) ? val : max_val;
    }

    // 2) Compute exponent sum
    float sum_exp = 0.0f;
    for(int c = 0; c < C; c++){
        float val = input[n*C*H*W + c*H*W + h*W + w];
        sum_exp += expf(val - max_val);
    }

    // 3) Write out final result: softmax -> + bias -> * scale -> sigmoid
    for(int c = 0; c < C; c++){
        float val = input[n*C*H*W + c*H*W + h*W + w];
        float softmax_val = expf(val - max_val) / sum_exp;
        float out_val = softmax_val + bias[c];  // bias is shape [C,1,1] but treated as flat
        out_val = out_val * scaling;
        out_val = 1.0f / (1.0f + expf(-out_val)); // sigmoid
        output[n*C*H*W + c*H*W + h*W + w] = out_val;
    }
}

torch::Tensor fused_softmax_bias_scale_sigmoid(
    torch::Tensor input,
    torch::Tensor bias,
    float scaling
){
    // input shape [N, C, H, W]
    // bias shape [C, 1, 1] (or effectively [C])
    int N = input.size(0);
    int C = input.size(1);
    int H = input.size(2);
    int W = input.size(3);

    auto output = torch::empty_like(input);

    const int threads = 256;
    const int blocks = (N * H * W + threads - 1) / threads;

    fused_softmax_bias_scale_sigmoid_kernel<<<blocks, threads>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        output.data_ptr<float>(),
        N, C, H, W, scaling
    );

    return output;
}
""";

fused_cpp_src = r"""
torch::Tensor fused_softmax_bias_scale_sigmoid(
    torch::Tensor input,
    torch::Tensor bias,
    float scaling
);
""";

# Build the fused custom kernel
fused_ops = load_inline(
    name="fused_ops",
    cpp_sources=fused_cpp_src,
    cuda_sources=fused_source,
    functions=["fused_softmax_bias_scale_sigmoid"],
    verbose=False
)

class ModelNew(nn.Module):
    """
    Optimized version of the model:
      ConvTranspose2d -> fused softmax+add bias+scale+sigmoid
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, bias_shape, scaling_factor):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            output_padding=output_padding
        )
        # Same bias param, but shape is [C,1,1]; used in fused kernel
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.scaling_factor = scaling_factor
        self.fused_softmax_bias_scale_sigmoid = fused_ops.fused_softmax_bias_scale_sigmoid

    def forward(self, x):
        # Keep the standard PyTorch conv_transpose
        x = self.conv_transpose(x)
        # Use fused custom kernel for softmax + bias + scale + sigmoid
        x = self.fused_softmax_bias_scale_sigmoid(x, self.bias, self.scaling_factor)
        return x
```