```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# The fused CUDA code replaces AvgPool2d + sigmoid + sum along [1,2,3]
# after a standard convolution. This kernel performs:
# 1) Average pooling (with stride=pool_size)
# 2) Sigmoid
# 3) Atomic sum reduction along channels & spatial dims, resulting in a
#    single value per batch element.
source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_pool_sigmoid_sum_kernel(const float* x, float* out,
                                              int B, int C, int H, int W,
                                              int pool_size) {
    int outH = H / pool_size;  // Assuming H is divisible by pool_size
    int outW = W / pool_size;  // Assuming W is divisible by pool_size

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total = B * C * outH * outW;

    if (idx < total) {
        int out_w = idx % outW;
        int tmp = idx / outW;
        int out_h = tmp % outH;
        tmp /= outH;
        int c = tmp % C;
        int b = tmp / C;

        // Compute start indices in the input
        int h_start = out_h * pool_size;
        int w_start = out_w * pool_size;

        // Perform average pooling
        float val_sum = 0.0f;
        for (int i = 0; i < pool_size; ++i) {
            for (int j = 0; j < pool_size; ++j) {
                int h_idx = h_start + i;
                int w_idx = w_start + j;
                val_sum += x[b * (C * H * W) + c * (H * W) + h_idx * W + w_idx];
            }
        }
        float avg_val = val_sum / (pool_size * pool_size);

        // Apply sigmoid
        float sig_val = 1.0f / (1.0f + expf(-avg_val));

        // Atomic add to accumulate the final sum over dim=[1,2,3]
        atomicAdd(&out[b], sig_val);
    }
}

torch::Tensor fused_pool_sigmoid_sum(torch::Tensor x, int pool_size) {
    // x shape: [B, C, H, W]
    auto B = x.size(0);
    auto C = x.size(1);
    auto H = x.size(2);
    auto W = x.size(3);

    // Prepare output shape: [B]
    auto out = torch::zeros({B}, x.options());

    // Launch CUDA kernel
    int outH = H / pool_size;
    int outW = W / pool_size;
    int total = B * C * outH * outW;
    const int blockSize = 256;
    const int gridSize = (total + blockSize - 1) / blockSize;

    fused_pool_sigmoid_sum_kernel<<<gridSize, blockSize>>>(
        x.data_ptr<float>(), out.data_ptr<float>(),
        B, C, (int)H, (int)W, pool_size
    );

    return out;
}
"""

# C++ declaration for our fused operator
cpp_src = r"""
torch::Tensor fused_pool_sigmoid_sum(torch::Tensor x, int pool_size);
"""

# Compile the inline CUDA extension
fused_ops = load_inline(
    name="fused_ops",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=["fused_pool_sigmoid_sum"],
    verbose=False,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    """
    This optimized model uses the standard PyTorch Conv2d, then
    calls a fused custom CUDA kernel to replace AvgPool + sigmoid + sum.
    """
    def __init__(self, in_channels, out_channels, kernel_size, pool_kernel_size):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.pool_kernel_size = pool_kernel_size
        self.fused_ops = fused_ops

    def forward(self, x):
        # Regular convolution
        x = self.conv(x)
        # Fused custom CUDA kernel for average pooling, sigmoid, and summation
        x = self.fused_ops.fused_pool_sigmoid_sum(x, self.pool_kernel_size)
        return x
```