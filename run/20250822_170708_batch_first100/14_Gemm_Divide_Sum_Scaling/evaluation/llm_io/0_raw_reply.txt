```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel source code
source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_forward_kernel(
    const float* __restrict__ x,
    const float* __restrict__ weight,
    float* __restrict__ out,
    int batch_size,
    int input_size,
    int hidden_size,
    float scale)
{
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < batch_size) {
        float sumVal = 0.0f;
        // Compute: sum_j( (x[i] dot weight[j]) / 2 )
        // then multiply by scale
        for (int j = 0; j < hidden_size; j++) {
            float dotVal = 0.0f;
            for (int k = 0; k < input_size; k++) {
                dotVal += x[i * input_size + k] * weight[j * input_size + k];
            }
            dotVal /= 2.0f;
            sumVal += dotVal;
        }
        sumVal *= scale;
        out[i] = sumVal;
    }
}

torch::Tensor fused_forward_cuda(torch::Tensor x, torch::Tensor weight, float scale) {
    // shapes:
    // x: (batch_size, input_size)
    // weight: (hidden_size, input_size)
    // output: (batch_size, 1)
    auto batch_size = x.size(0);
    auto input_size = x.size(1);
    auto hidden_size = weight.size(0);

    auto out = torch::empty({batch_size}, x.options());

    int block_size = 256;
    int grid_size = (batch_size + block_size - 1) / block_size;

    fused_forward_kernel<<<grid_size, block_size>>>(
        x.data_ptr<float>(),
        weight.data_ptr<float>(),
        out.data_ptr<float>(),
        batch_size,
        input_size,
        hidden_size,
        scale
    );

    return out.view({batch_size, 1});
}
"""

cpp_src = r"""
torch::Tensor fused_forward_cuda(torch::Tensor x, torch::Tensor weight, float scale);
"""

# Load and compile the inline CUDA extension
fused_op = load_inline(
    name="fused_op",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=["fused_forward_cuda"],
    verbose=False,
)

class ModelNew(nn.Module):
    """
    Optimized model that fuses Gemm, division, sum, and scaling into one custom CUDA kernel.
    """
    def __init__(self, input_size, hidden_size, scaling_factor):
        super(ModelNew, self).__init__()
        self.weight = nn.Parameter(torch.randn(hidden_size, input_size))
        self.scaling_factor = scaling_factor

    def forward(self, x):
        return fused_op.fused_forward_cuda(x, self.weight, self.scaling_factor)

batch_size   = 1024
input_size   = 8192
hidden_size  = 8192
scaling_factor = 1.5

def get_inputs():
    return [torch.rand(batch_size, input_size).cuda()]

def get_init_inputs():
    return [input_size, hidden_size, scaling_factor]
```
