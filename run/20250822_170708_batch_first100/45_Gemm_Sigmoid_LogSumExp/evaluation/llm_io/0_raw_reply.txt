```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Inline CUDA source with three custom kernels:
# 1) Fused GEMM + Sigmoid
# 2) GEMM (no activation)
# 3) LogSumExp (dimension=1)
source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

// 1) Fused GEMM + Sigmoid kernel
__global__ void gemm_sigmoid_kernel(const float* __restrict__ x,
                                    const float* __restrict__ w,
                                    const float* __restrict__ b,
                                    float* __restrict__ out,
                                    int N, int K, int M) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    if (row < N && col < M) {
        float val = 0.0f;
        for (int i = 0; i < K; i++) {
            val += x[row * K + i] * w[col * K + i];
        }
        val += b[col];
        // Sigmoid
        val = 1.0f / (1.0f + expf(-val));
        out[row * M + col] = val;
    }
}

torch::Tensor gemm_sigmoid_cuda(torch::Tensor x, 
                                torch::Tensor w, 
                                torch::Tensor b) {
    const auto N = x.size(0);
    const auto K = x.size(1);
    const auto M = w.size(0);
    auto out = torch::empty({N, M}, x.options());

    dim3 block(16, 16);
    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);

    gemm_sigmoid_kernel<<<grid, block>>>(x.data_ptr<float>(),
                                         w.data_ptr<float>(),
                                         b.data_ptr<float>(),
                                         out.data_ptr<float>(),
                                         N, K, M);
    return out;
}

// 2) GEMM (no activation) kernel
__global__ void gemm_kernel(const float* __restrict__ x,
                            const float* __restrict__ w,
                            const float* __restrict__ b,
                            float* __restrict__ out,
                            int N, int K, int M) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    if (row < N && col < M) {
        float val = 0.0f;
        for (int i = 0; i < K; i++) {
            val += x[row * K + i] * w[col * K + i];
        }
        val += b[col];
        out[row * M + col] = val;
    }
}

torch::Tensor gemm_cuda(torch::Tensor x, 
                        torch::Tensor w, 
                        torch::Tensor b) {
    const auto N = x.size(0);
    const auto K = x.size(1);
    const auto M = w.size(0);
    auto out = torch::empty({N, M}, x.options());

    dim3 block(16, 16);
    dim3 grid((M + block.x - 1) / block.x, (N + block.y - 1) / block.y);

    gemm_kernel<<<grid, block>>>(x.data_ptr<float>(),
                                 w.data_ptr<float>(),
                                 b.data_ptr<float>(),
                                 out.data_ptr<float>(),
                                 N, K, M);
    return out;
}

// 3) LogSumExp kernel (dim=1)
__global__ void logsumexp_kernel(const float* __restrict__ inp,
                                 float* __restrict__ out,
                                 int N, int M) {
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    if (row < N) {
        // Find max for stability
        float max_val = -1e30f;
        for (int i = 0; i < M; i++) {
            float val = inp[row * M + i];
            if (val > max_val) max_val = val;
        }
        // Sum exp
        float sum_exp = 0.0f;
        for (int i = 0; i < M; i++) {
            sum_exp += expf(inp[row * M + i] - max_val);
        }
        out[row] = logf(sum_exp) + max_val;
    }
}

torch::Tensor logsumexp_cuda(torch::Tensor x, int dim) {
    // For this custom kernel, we only handle dim=1:
    TORCH_CHECK(x.dim() == 2, "Input must be 2D for logsumexp_cuda");
    TORCH_CHECK(dim == 1, "logsumexp_cuda only supports dim=1");
    const auto N = x.size(0);
    const auto M = x.size(1);
    auto out = torch::empty({N}, x.options());

    int block = 256;
    int grid = (N + block - 1) / block;

    logsumexp_kernel<<<grid, block>>>(x.data_ptr<float>(),
                                      out.data_ptr<float>(),
                                      N, M);
    return out;
}
"""

# Declarations for the three custom functions
cpp_src = r"""
torch::Tensor gemm_sigmoid_cuda(torch::Tensor x, torch::Tensor w, torch::Tensor b);
torch::Tensor gemm_cuda(torch::Tensor x, torch::Tensor w, torch::Tensor b);
torch::Tensor logsumexp_cuda(torch::Tensor x, int dim);
"""

# Build the custom operators
custom_ops = load_inline(
    name="custom_gemm_ops",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=["gemm_sigmoid_cuda", "gemm_cuda", "logsumexp_cuda"],
    verbose=False,
)

# New optimized model using custom CUDA kernels
class ModelNew(nn.Module):
    """
    Optimized model that replaces:
     1) Linear1 + Sigmoid with fused gemm_sigmoid
     2) Linear2 with gemm
     3) logsumexp with a custom logsumexp kernel
    """
    def __init__(self, input_size, hidden_size, output_size):
        super(ModelNew, self).__init__()
        # We'll still hold nn.Linear so we can keep trainable parameters,
        # but we won't use its forward(). Instead, we call our fused kernels.
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # GEMM + Sigmoid
        x = custom_ops.gemm_sigmoid_cuda(x, self.linear1.weight, self.linear1.bias)
        # GEMM
        x = custom_ops.gemm_cuda(x, self.linear2.weight, self.linear2.bias)
        # LogSumExp over dim=1
        x = custom_ops.logsumexp_cuda(x, 1)
        return x

batch_size = 16384
input_size = 2048
hidden_size = 4096
output_size = 1024

def get_inputs():
    return [torch.rand(batch_size, input_size).cuda()]

def get_init_inputs():
    return [input_size, hidden_size, output_size]
```