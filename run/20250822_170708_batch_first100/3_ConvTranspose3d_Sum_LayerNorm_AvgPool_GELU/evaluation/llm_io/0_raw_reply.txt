```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom fused CUDA kernel: adds a scalar (sum_weight) and applies GELU
source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>

__device__ __forceinline__ float gelu(const float x) {
    const float kAlpha = 0.7978845608f;      // sqrt(2 / pi)
    const float kGamma = 0.044715f;
    float t = x * (1.0f + kGamma * x * x);
    float cdf = 0.5f * (1.0f + tanhf(kAlpha * t));
    return x * cdf;
}

__global__ void fused_add_gelu_kernel(const float* __restrict__ inp,
                                      const float sum_weight,
                                      float* __restrict__ out,
                                      int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float val = inp[idx] + sum_weight;
        out[idx] = gelu(val);
    }
}

torch::Tensor fused_add_gelu_cuda(torch::Tensor x, float sum_weight) {
    auto out = torch::empty_like(x);
    int size = x.numel();

    const int block_size = 256;
    const int grid_size = (size + block_size - 1) / block_size;

    fused_add_gelu_kernel<<<grid_size, block_size>>>(
        x.data_ptr<float>(),
        sum_weight,
        out.data_ptr<float>(),
        size
    );

    return out;
}
""";

cpp_src = r"""
torch::Tensor fused_add_gelu_cuda(torch::Tensor x, float sum_weight);
""";

fused_ops = load_inline(
    name="fused_add_gelu",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=["fused_add_gelu_cuda"],
    verbose=False
)


class ModelNew(nn.Module):
    """
    Optimized Model that:
    1) Performs a 3D transposed convolution
    2) Layer normalization
    3) Average pooling
    4) Aggregates sum_weight and applies GELU in a single fused CUDA kernel
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, sum_weight, norm_shape, pool_kernel_size):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding, output_padding=output_padding
        )
        self.sum_weight = nn.Parameter(torch.tensor(sum_weight, dtype=torch.float32))
        self.norm = nn.LayerNorm(norm_shape)
        self.avg_pool = nn.AvgPool3d(kernel_size=pool_kernel_size)
        # This GELU module is kept to align with the original architecture's parameters
        self.gelu = nn.GELU()  
        self.fused_add_gelu = fused_ops

    def forward(self, x):
        # 1) 3D transposed convolution
        x = self.conv_transpose(x)
        # 2) Layer normalization
        x = self.norm(x)
        # 3) Average pooling
        x = self.avg_pool(x)
        # 4) Fused addition + GELU
        x = self.fused_add_gelu.fused_add_gelu_cuda(x, self.sum_weight.item())
        return x
```