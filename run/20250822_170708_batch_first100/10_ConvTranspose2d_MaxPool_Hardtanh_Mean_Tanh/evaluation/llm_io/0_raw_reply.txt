```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Inline CUDA/C++ source for fused custom operators: maxpool+hardtanh and mean+tanh
source = r'''
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <cmath>

// -------------------------------------------
// Kernel 1: maxpool + hardtanh
// -------------------------------------------
__global__ void maxpool_hardtanh_kernel(
    const float* __restrict__ input, 
    float* __restrict__ output,
    int bs, int ch, int height, int width,
    int pool_kernel, int pool_stride,
    float min_val, float max_val) 
{
    // Output height/width after pooling
    int out_height = height / pool_stride;
    int out_width  = width / pool_stride;

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total = bs * ch * out_height * out_width;
    if (idx >= total) return;

    // Decompose idx into b, c, oh, ow
    int ow = idx % out_width;
    int oh = (idx / out_width) % out_height;
    int c  = (idx / (out_width * out_height)) % ch;
    int b  = idx / (out_width * out_height * ch);

    // Region in input
    int in_h_start = oh * pool_stride;
    int in_w_start = ow * pool_stride;

    float max_val_region = -FLT_MAX;
    for(int kh = 0; kh < pool_kernel; kh++){
        for(int kw = 0; kw < pool_kernel; kw++){
            int h_pos = in_h_start + kh;
            int w_pos = in_w_start + kw;
            int in_index = b * ch * height * width 
                           + c * height * width 
                           + h_pos * width 
                           + w_pos;
            float val = input[in_index];
            if (val > max_val_region){
                max_val_region = val;
            }
        }
    }

    // HardTanh clamp
    if (max_val_region < min_val) {
        max_val_region = min_val;
    } else if (max_val_region > max_val) {
        max_val_region = max_val;
    }

    // Write to output
    int out_index = idx;
    output[out_index] = max_val_region;
}

torch::Tensor custom_maxpool_hardtanh_cuda(
    torch::Tensor input, 
    int pool_kernel, 
    int pool_stride, 
    float min_val, 
    float max_val) 
{
    // Assume input is NCHW
    int bs = input.size(0);
    int ch = input.size(1);
    int h  = input.size(2);
    int w  = input.size(3);

    // Output shape
    int out_h = h / pool_stride; 
    int out_w = w / pool_stride;

    auto options = input.options();
    auto out = torch::empty({bs, ch, out_h, out_w}, options);

    int total = bs * ch * out_h * out_w;

    const int block_size = 256;
    const int grid_size = (total + block_size - 1) / block_size;

    maxpool_hardtanh_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        out.data_ptr<float>(),
        bs, ch, h, w,
        pool_kernel, pool_stride,
        min_val, max_val
    );

    return out;
}

// -------------------------------------------
// Kernel 2: mean over spatial dims + tanh
// -------------------------------------------
__global__ void mean_tanh_kernel(
    const float* __restrict__ input,
    float* __restrict__ output,
    int bs, int ch, int height, int width)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total = bs * ch;
    if (idx >= total) return;

    int c  = idx % ch;
    int b  = idx / ch;

    // Sum over all spatial
    long start_idx = (long)b * ch * height * width + (long)c * height * width;
    float sum_val = 0.0f;
    for(int i = 0; i < height * width; i++){
        sum_val += input[start_idx + i];
    }
    float mean_val = sum_val / static_cast<float>(height * width);

    // Apply tanh
    float out_val = tanhf(mean_val);

    // Store in [bs, ch]
    output[idx] = out_val;
}

torch::Tensor custom_mean_tanh_cuda(torch::Tensor input) 
{
    // input shape is [N, C, H, W]
    int bs = input.size(0);
    int ch = input.size(1);
    int h  = input.size(2);
    int w  = input.size(3);

    auto options = input.options();
    auto out = torch::empty({bs, ch}, options);

    int total = bs * ch;
    const int block_size = 256;
    const int grid_size = (total + block_size - 1) / block_size;

    mean_tanh_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        out.data_ptr<float>(),
        bs, ch, h, w
    );

    return out;
}
'''

cpp_src = r'''
torch::Tensor custom_maxpool_hardtanh_cuda(torch::Tensor input, int pool_kernel, int pool_stride, float min_val, float max_val);
torch::Tensor custom_mean_tanh_cuda(torch::Tensor input);
'''

# Build the fused CUDA module
custom_ops = load_inline(
    name="custom_ops",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=["custom_maxpool_hardtanh_cuda", "custom_mean_tanh_cuda"],
    verbose=False
)

class ModelNew(nn.Module):
    """
    Optimized model with custom CUDA operators for maxpool+hardtanh and mean+tanh.
    """
    def __init__(
        self, 
        in_channels, 
        out_channels, 
        kernel_size, 
        stride, 
        padding, 
        maxpool_kernel_size, 
        maxpool_stride, 
        hardtanh_min, 
        hardtanh_max
    ):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels, out_channels, kernel_size, 
            stride=stride, padding=padding
        )
        self.maxpool_kernel_size = maxpool_kernel_size
        self.maxpool_stride = maxpool_stride
        self.hardtanh_min = hardtanh_min
        self.hardtanh_max = hardtanh_max
        self.custom_ops = custom_ops

    def forward(self, x):
        # Standard PyTorch transposed convolution
        x = self.conv_transpose(x)
        # Custom fused maxpool + hardtanh
        x = self.custom_ops.custom_maxpool_hardtanh_cuda(
            x, 
            self.maxpool_kernel_size, 
            self.maxpool_stride, 
            self.hardtanh_min, 
            self.hardtanh_max
        )
        # Custom mean over spatial + tanh
        x = self.custom_ops.custom_mean_tanh_cuda(x)
        # Reshape [N, C] -> [N, C, 1, 1]
        x = x.view(x.size(0), x.size(1), 1, 1)
        return x
```
