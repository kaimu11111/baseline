```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Inline CUDA code for fused GELU + GroupNorm
source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__device__ __forceinline__ float gelu(float x) {
    const float kAlpha = 0.7978845608f; // sqrt(2/pi)
    const float kCoeff = 0.044715f;
    float y = x * (1.0f + kCoeff * x * x);
    return 0.5f * x * (1.0f + tanhf(kAlpha * y));
}

__global__ void fused_gelu_sum_kernel(
    const float* __restrict__ x,
    float* __restrict__ out,
    float* __restrict__ sum,
    float* __restrict__ sqSum,
    int N, int C, int H, int W, int group_size)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total = N * C * H * W;
    if (idx < total) {
        float val = x[idx];
        // Apply GELU
        val = gelu(val);
        out[idx] = val;

        int n = idx / (C * H * W);
        int c = (idx % (C * H * W)) / (H * W);
        int group_idx = c / group_size;

        // Atomic adds for sum and sqSum
        atomicAdd(&sum[n * (C / group_size) + group_idx], val);
        atomicAdd(&sqSum[n * (C / group_size) + group_idx], val * val);
    }
}

__global__ void groupnorm_kernel(
    float* __restrict__ out,
    const float* __restrict__ sum,
    const float* __restrict__ sqSum,
    const float* __restrict__ gamma,
    const float* __restrict__ beta,
    int N, int C, int H, int W,
    int group_size, float eps)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total = N * C * H * W;
    if (idx < total) {
        int n = idx / (C * H * W);
        int c = (idx % (C * H * W)) / (H * W);
        int group_idx = c / group_size;

        float mean = sum[n * (C / group_size) + group_idx] / (group_size * H * W);
        float sqmean = sqSum[n * (C / group_size) + group_idx] / (group_size * H * W);
        float var = sqmean - mean * mean;
        float inv_std = rsqrtf(var + eps);

        float val = out[idx];
        float g = gamma[c];
        float b = beta[c];
        val = (val - mean) * inv_std * g + b;
        out[idx] = val;
    }
}

torch::Tensor fused_gelu_groupnorm_forward(
    torch::Tensor x,
    torch::Tensor gamma,
    torch::Tensor beta,
    int num_groups,
    float eps)
{
    const auto N = x.size(0);
    const auto C = x.size(1);
    const auto H = x.size(2);
    const auto W = x.size(3);

    int group_size = C / num_groups;
    auto out = torch::empty_like(x);

    auto sum = torch::zeros({N, num_groups}, x.options());
    auto sqSum = torch::zeros({N, num_groups}, x.options());

    int total = N * C * H * W;
    int block_size = 256;
    int grid_size = (total + block_size - 1) / block_size;

    fused_gelu_sum_kernel<<<grid_size, block_size>>>(
        x.data_ptr<float>(),
        out.data_ptr<float>(),
        sum.data_ptr<float>(),
        sqSum.data_ptr<float>(),
        N, C, H, W, group_size
    );

    groupnorm_kernel<<<grid_size, block_size>>>(
        out.data_ptr<float>(),
        sum.data_ptr<float>(),
        sqSum.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        N, C, H, W, group_size, eps
    );

    return out;
}
""";

cpp_src = r"""
torch::Tensor fused_gelu_groupnorm_forward(
    torch::Tensor x,
    torch::Tensor gamma,
    torch::Tensor beta,
    int num_groups,
    float eps);
""";

# Compile the extension
fused_mod = load_inline(
    name="fused_gelu_groupnorm_mod",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=["fused_gelu_groupnorm_forward"],
    verbose=False
)


class ModelNew(nn.Module):
    """
    Optimized Model that uses PyTorch's ConvTranspose2d and a fused custom CUDA kernel
    for GELU + GroupNorm.
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, groups, num_groups):
        super(ModelNew, self).__init__()
        # Keep the same transposed conv
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, groups=groups)
        # Create separate gamma and beta (GroupNorm weight and bias)
        self.gamma = nn.Parameter(torch.ones(out_channels))
        self.beta = nn.Parameter(torch.zeros(out_channels))
        self.num_groups = num_groups
        # Use the same default eps as PyTorch's GroupNorm
        self.eps = 1e-5

    def forward(self, x):
        x = self.conv_transpose(x)
        x = fused_mod.fused_gelu_groupnorm_forward(x, self.gamma, self.beta, self.num_groups, self.eps)
        return x


batch_size   = 128
in_channels  = 64
out_channels = 64
height = width = 256
kernel_size  = 3
stride       = 1
groups = 8
num_groups = 8

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width).cuda()]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, groups, num_groups]
```