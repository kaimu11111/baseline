```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Inline CUDA code that fuses batch norm + tanh and also implements max pooling
source = r"""
#include <torch/extension.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

// Kernel for fused BatchNorm (inference mode) + Tanh
// x, weight, bias, mean, var: all float tensors
// out: float tensor (same shape as x)
// The shape is assumed to be (N, C, H, W).
// For each element, we do:
//   norm = (x - mean[c]) / sqrt(var[c] + eps) * weight[c] + bias[c]
//   out = tanh(norm)
__global__ void bn_tanh_kernel(
    const float* __restrict__ x,
    const float* __restrict__ weight,
    const float* __restrict__ bias,
    const float* __restrict__ running_mean,
    const float* __restrict__ running_var,
    float* __restrict__ out,
    const float eps,
    const int N,
    const int C,
    const int H,
    const int W)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total = N * C * H * W;
    if (idx < total) {
        // figure out which channel
        int c = (idx / (H * W)) % C;
        float val = x[idx];
        float mean_val = running_mean[c];
        float var_val = running_var[c];
        float w = weight[c];
        float b = bias[c];

        float norm = (val - mean_val) / sqrtf(var_val + eps);
        norm = norm * w + b;
        out[idx] = tanhf(norm);
    }
}

// Kernel for 2D max pooling of kernel_size=2, stride=2
// We assume input shape: (N, C, H, W)
// Output shape: (N, C, H/2, W/2) (rounded down if odd, simple for this example)
__global__ void max_pool_kernel(
    const float* __restrict__ in,
    float* __restrict__ out,
    const int N,
    const int C,
    const int H,
    const int W)
{
    // Output height/width
    int outH = H / 2;
    int outW = W / 2;

    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total = N * C * outH * outW;
    if (idx < total) {
        // decode (n, c, h_out, w_out)
        int w_out = idx % outW;
        int h_out = (idx / outW) % outH;
        int c     = (idx / (outW * outH)) % C;
        int n     = idx / (outW * outH * C);

        // input origin
        int h_in = h_out * 2;
        int w_in = w_out * 2;

        float max_val = -FLT_MAX;
        for(int i = 0; i < 2; i++){
            for(int j = 0; j < 2; j++){
                int h_cur = h_in + i;
                int w_cur = w_in + j;
                float val = in[((n*C + c)*H + h_cur)*W + w_cur];
                if (val > max_val) {
                    max_val = val;
                }
            }
        }
        out[idx] = max_val;
    }
}

torch::Tensor bn_tanh_cuda(
    torch::Tensor x,
    torch::Tensor weight,
    torch::Tensor bias,
    torch::Tensor running_mean,
    torch::Tensor running_var,
    float eps)
{
    TORCH_CHECK(x.dim() == 4, "Input must be 4D");
    auto N = x.size(0);
    auto C = x.size(1);
    auto H = x.size(2);
    auto W = x.size(3);

    auto out = torch::empty_like(x);

    int total = N * C * H * W;
    int block_size = 256;
    int grid_size = (total + block_size - 1) / block_size;

    bn_tanh_kernel<<<grid_size, block_size>>>(
        x.data_ptr<float>(), 
        weight.data_ptr<float>(),
        bias.data_ptr<float>(), 
        running_mean.data_ptr<float>(),
        running_var.data_ptr<float>(),
        out.data_ptr<float>(),
        eps,
        N, C, H, W
    );

    return out;
}

torch::Tensor max_pool_cuda(torch::Tensor x)
{
    TORCH_CHECK(x.dim() == 4, "Input must be 4D");
    auto N = x.size(0);
    auto C = x.size(1);
    auto H = x.size(2);
    auto W = x.size(3);

    int outH = H / 2;
    int outW = W / 2;

    auto out = torch::empty({N, C, outH, outW}, x.options());

    int total = N * C * outH * outW;
    int block_size = 256;
    int grid_size = (total + block_size - 1) / block_size;

    max_pool_kernel<<<grid_size, block_size>>>(
        x.data_ptr<float>(),
        out.data_ptr<float>(),
        N, C, H, W
    );

    return out;
}
""";

cpp_hdr = r"""
torch::Tensor bn_tanh_cuda(
    torch::Tensor x,
    torch::Tensor weight,
    torch::Tensor bias,
    torch::Tensor running_mean,
    torch::Tensor running_var,
    float eps
);

torch::Tensor max_pool_cuda(torch::Tensor x);
""";

# Build the fused custom operators
cudafused = load_inline(
    name="cudafused_ops",
    cpp_sources=cpp_hdr,
    cuda_sources=source,
    extra_cflags=["-O3"],
    extra_cuda_cflags=["--use_fast_math", "-O3"],
    verbose=False,
    functions=["bn_tanh_cuda", "max_pool_cuda"],
)

class ModelNew(nn.Module):
    """
    Optimized model that uses custom CUDA kernels for fused BatchNorm + Tanh and MaxPool.
    The transposed convolution and group norm are left as PyTorch ops.
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups, num_groups):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding
        )
        self.batch_norm = nn.BatchNorm2d(out_channels)
        # GroupNorm remains as normal
        self.group_norm = nn.GroupNorm(num_groups=num_groups, num_channels=out_channels)
        self.cudafused = cudafused

    def forward(self, x):
        # Standard transposed conv
        x = self.conv_transpose(x)
        # Fused BN + Tanh
        x = self.cudafused.bn_tanh_cuda(
            x,
            self.batch_norm.weight,
            self.batch_norm.bias,
            self.batch_norm.running_mean,
            self.batch_norm.running_var,
            self.batch_norm.eps
        )
        # Fused MaxPool
        x = self.cudafused.max_pool_cuda(x)
        # GroupNorm as normal
        x = self.group_norm(x)
        return x


batch_size = 512
in_channels  = 64
out_channels = 128
height = width = 2048
kernel_size  = 5
stride       = 1
padding      = 1
groups       = 8
num_groups   = 8
height, width = 32, 32

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width).cuda()]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding, groups, num_groups]
```