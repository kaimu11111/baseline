```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <vector>

// --------------------------------------
// 3D MaxPool with kernel_size=3, stride=3
// --------------------------------------
__global__ void maxpool3d_kernel(
    const float* __restrict__ input,
    float* __restrict__ output,
    int N, int C,
    int D_in, int H_in, int W_in,
    int D_out, int H_out, int W_out)
{
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    int total = N * C * D_out * H_out * W_out;
    if (index < total) {
        // Decompose linear index into (n, c, d_out, h_out, w_out)
        int w_o = index % W_out;
        int tmp = index / W_out;
        int h_o = tmp % H_out;
        tmp /= H_out;
        int d_o = tmp % D_out;
        tmp /= D_out;
        int c = tmp % C;
        int n = tmp / C;

        // Compute the input region start/end
        int d_start = d_o * 3;
        int d_end = min(d_start + 3, D_in);
        int h_start = h_o * 3;
        int h_end = min(h_start + 3, H_in);
        int w_start = w_o * 3;
        int w_end = min(w_start + 3, W_in);

        float max_val = -FLT_MAX;
        for (int d = d_start; d < d_end; d++) {
            for (int h = h_start; h < h_end; h++) {
                for (int w = w_start; w < w_end; w++) {
                    int in_idx = n * (C * D_in * H_in * W_in)
                               + c * (D_in * H_in * W_in)
                               + d * (H_in * W_in)
                               + h * (W_in)
                               + w;
                    float val = input[in_idx];
                    if (val > max_val) {
                        max_val = val;
                    }
                }
            }
        }
        output[index] = max_val;
    }
}

torch::Tensor maxpool3d_cuda(torch::Tensor input) {
    // Input shape: [N, C, D_in, H_in, W_in]
    int N = input.size(0);
    int C = input.size(1);
    int D_in = input.size(2);
    int H_in = input.size(3);
    int W_in = input.size(4);

    // kernel_size=3, stride=3, no padding
    int D_out = D_in / 3;
    int H_out = H_in / 3;
    int W_out = W_in / 3;

    auto options = input.options();
    auto output = torch::empty({N, C, D_out, H_out, W_out}, options);

    int total = N * C * D_out * H_out * W_out;
    int block_size = 256;
    int grid_size = (total + block_size - 1) / block_size;

    maxpool3d_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        N, C,
        D_in, H_in, W_in,
        D_out, H_out, W_out
    );
    return output;
}

// --------------------------------------
// Sum over dim=1 (C dimension) keeping dim
// --------------------------------------
__global__ void sum_dim1_kernel(
    const float* __restrict__ input,
    float* __restrict__ output,
    int N, int C,
    int D, int H, int W)
{
    // Output shape: [N, 1, D, H, W], same as input except C=1
    int index = blockIdx.x * blockDim.x + threadIdx.x;
    int total = N * D * H * W; // since output has C=1
    if (index < total) {
        // Decompose linear index into (n, d, h, w)
        int w_o = index % W;
        int tmp = index / W;
        int h_o = tmp % H;
        tmp /= H;
        int d_o = tmp % D;
        int n = tmp / D;

        float sum_val = 0.0f;
        for (int c = 0; c < C; c++) {
            int in_idx = n * (C * D * H * W)
                       + c * (D * H * W)
                       + d_o * (H * W)
                       + h_o * W
                       + w_o;
            sum_val += input[in_idx];
        }
        // Output is [n, 0, d_o, h_o, w_o]
        int out_idx = n * (1 * D * H * W)
                    + 0 * (D * H * W)
                    + d_o * (H * W)
                    + h_o * W
                    + w_o;
        output[out_idx] = sum_val;
    }
}

torch::Tensor sum_dim1_cuda(torch::Tensor input) {
    // Input shape: [N, C, D, H, W]
    int N = input.size(0);
    int C = input.size(1);
    int D = input.size(2);
    int H = input.size(3);
    int W = input.size(4);

    auto options = input.options();
    auto output = torch::empty({N, 1, D, H, W}, options);

    int total = N * D * H * W;
    int block_size = 256;
    int grid_size = (total + block_size - 1) / block_size;

    sum_dim1_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        output.data_ptr<float>(),
        N, C, D, H, W
    );
    return output;
}
""";

cpp_src = r"""
torch::Tensor maxpool3d_cuda(torch::Tensor input);
torch::Tensor sum_dim1_cuda(torch::Tensor input);
""";

custom_ops = load_inline(
    name="custom_ops_3d",
    cpp_sources=cpp_src,
    cuda_sources=source,
    extra_cflags=[],
    extra_ldflags=[],
    verbose=False,
    functions=["maxpool3d_cuda", "sum_dim1_cuda"]
)

class ModelNew(nn.Module):
    """
    Optimized model that replaces the second 3D max pooling and the sum operation with
    custom CUDA kernels.
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):
        super(ModelNew, self).__init__()
        # Keep the regular transpose convolution and first pool
        self.conv_transpose = nn.ConvTranspose3d(in_channels, out_channels, kernel_size,
                                                 stride=stride, padding=padding)
        self.max_pool1 = nn.MaxPool3d(kernel_size=2)
        self.custom_ops = custom_ops

    def forward(self, x):
        x = self.conv_transpose(x)
        x = self.max_pool1(x)
        # Use our custom kernel for the second maxpool (kernel=3, stride=3)
        x = self.custom_ops.maxpool3d_cuda(x)
        # Sum over dim=1, keepdim=True
        x = self.custom_ops.sum_dim1_cuda(x)
        return x

batch_size = 16
in_channels = 32
out_channels = 64
depth, height, width = 32, 32, 32
kernel_size = 5
stride = 2
padding = 2

def get_inputs():
    return [torch.rand(batch_size, in_channels, depth, height, width).cuda()]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, stride, padding]
```