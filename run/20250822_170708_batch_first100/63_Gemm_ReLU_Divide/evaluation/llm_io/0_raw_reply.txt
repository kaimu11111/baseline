```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Fused kernel: performs X * W^T + bias, then ReLU, then divides by divisor
source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_linear_relu_div_kernel(
    const float* __restrict__ X,
    const float* __restrict__ W,
    const float* __restrict__ bias,
    float* __restrict__ out,
    int batch_size,
    int in_features,
    int out_features,
    float divisor)
{
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    if (row < batch_size && col < out_features) {
        float val = 0.0f;
        for(int i = 0; i < in_features; i++){
            val += X[row * in_features + i] * W[col * in_features + i];
        }
        val += bias[col];
        // ReLU
        if (val < 0.0f) {
            val = 0.0f;
        }
        // Divide
        val /= divisor;
        out[row * out_features + col] = val;
    }
}

torch::Tensor fused_linear_relu_div_cuda(
    torch::Tensor X,
    torch::Tensor W,
    torch::Tensor bias,
    float divisor)
{
    // Assume X: (batch_size, in_features), W: (out_features, in_features), bias: (out_features)
    auto batch_size = X.size(0);
    auto in_features = X.size(1);
    auto out_features = W.size(0);
    
    auto out = torch::zeros({batch_size, out_features}, X.options());
    
    dim3 block(16, 16);
    dim3 grid((out_features + block.x - 1) / block.x,
              (batch_size + block.y - 1) / block.y);

    fused_linear_relu_div_kernel<<<grid, block>>>(
        X.data_ptr<float>(),
        W.data_ptr<float>(),
        bias.data_ptr<float>(),
        out.data_ptr<float>(),
        batch_size,
        in_features,
        out_features,
        divisor);

    return out;
}
"""

cpp_src = """
torch::Tensor fused_linear_relu_div_cuda(
    torch::Tensor X,
    torch::Tensor W,
    torch::Tensor bias,
    float divisor
);
"""

fused_ops = load_inline(
    name="fused_ops",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=["fused_linear_relu_div_cuda"],
    verbose=False
)

class ModelNew(nn.Module):
    """
    Optimized model that fuses the linear, ReLU, and division steps into a single CUDA kernel.
    """
    def __init__(self, in_features, out_features, divisor):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(out_features, in_features, device='cuda'))
        self.bias = nn.Parameter(torch.randn(out_features, device='cuda'))
        self.divisor = divisor

    def forward(self, x):
        return fused_ops.fused_linear_relu_div_cuda(x, self.weight, self.bias, self.divisor)

def get_inputs():
    return [torch.rand(1024, 8192, device='cuda')]

def get_init_inputs():
    return [8192, 8192, 2.0]
```