```python
import math
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <cmath>

__global__ void fused_forward_kernel(const float* __restrict__ x, 
                                     const float* __restrict__ w, 
                                     const float* __restrict__ b,
                                     float* __restrict__ out,
                                     int B, int in_features, int out_features, float scaling_factor) {
    int idx = blockDim.x * blockIdx.x + threadIdx.x;
    if (idx < B * out_features) {
        int i = idx / out_features;  
        int j = idx % out_features;  
        float val = 0.0f;
        for (int k = 0; k < in_features; k++) {
            val += x[i * in_features + k] * w[j * in_features + k];
        }
        val += b[j];
        float sig = 1.0f / (1.0f + expf(-val));
        val = val * sig;
        val *= scaling_factor;
        out[i * out_features + j] = val;
    }
}

torch::Tensor fused_forward_cuda(torch::Tensor x, 
                                 torch::Tensor w, 
                                 torch::Tensor b,
                                 float scaling_factor) {
    TORCH_CHECK(x.dim() == 2, "x must be 2D");
    TORCH_CHECK(w.dim() == 2, "w must be 2D");
    TORCH_CHECK(b.dim() == 1, "b must be 1D");
    int B = x.size(0);
    int in_features = x.size(1);
    int out_features = w.size(0); 
    TORCH_CHECK(w.size(1) == in_features, "weight shape mismatch");
    TORCH_CHECK(b.size(0) == out_features, "bias shape mismatch");
    auto out = torch::empty({B, out_features}, x.options());

    const int threads = 256;
    const int blocks = (B * out_features + threads - 1) / threads;
    fused_forward_kernel<<<blocks, threads>>>(
        x.data_ptr<float>(),
        w.data_ptr<float>(),
        b.data_ptr<float>(),
        out.data_ptr<float>(),
        B, in_features, out_features, scaling_factor
    );
    return out;
}
"""

cpp_src = r"""
torch::Tensor fused_forward_cuda(torch::Tensor x, 
                                 torch::Tensor w, 
                                 torch::Tensor b,
                                 float scaling_factor);
"""

fused_ext = load_inline(
    name="fused_ext",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=["fused_forward_cuda"],
    verbose=False
)

class ModelNew(nn.Module):
    """
    Optimized model that fuses matmul, Swish activation and scaling into a single CUDA kernel.
    """
    def __init__(self, in_features, out_features, scaling_factor):
        super(ModelNew, self).__init__()
        self.weight = nn.Parameter(torch.empty(out_features, in_features))
        self.bias = nn.Parameter(torch.empty(out_features))
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
        bound = 1 / math.sqrt(fan_in)
        nn.init.uniform_(self.bias, -bound, bound)
        self.scaling_factor = scaling_factor

    def forward(self, x):
        return fused_ext.fused_forward_cuda(x, self.weight, self.bias, self.scaling_factor)
```