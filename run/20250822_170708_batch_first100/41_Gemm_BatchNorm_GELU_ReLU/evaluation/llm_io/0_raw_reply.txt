```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA kernel with fused BatchNorm, GeLU, and ReLU
source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <cmath>

__device__ __forceinline__ float approximate_gelu(float x) {
    // Approximate GeLU from OpenAI GPT-2
    // 0.5 * x * (1 + tanh( sqrt(2/pi) * (x + 0.044715 * x^3 ) ) )
    const float M_SQRT_2_PI = 0.7978845608f; // sqrt(2/pi)
    float x3 = x * x * x;
    float inner = M_SQRT_2_PI * (x + 0.044715f * x3);
    return 0.5f * x * (1.0f + tanhf(inner));
}

__global__ void bn_gelu_relu_kernel(
    const float* __restrict__ x,
    float* __restrict__ out,
    const float* __restrict__ gamma,
    const float* __restrict__ beta,
    const float* __restrict__ mean,
    const float* __restrict__ var,
    const float eps,
    const int batch_size,
    const int features
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total = batch_size * features;
    if (idx < total) {
        int f = idx % features;
        float normed = (x[idx] - mean[f]) / sqrtf(var[f] + eps);
        normed = gamma[f] * normed + beta[f];
        float val_gelu = approximate_gelu(normed);
        // Apply ReLU
        out[idx] = fmaxf(val_gelu, 0.0f);
    }
}

torch::Tensor bn_gelu_relu_cuda(
    torch::Tensor x,
    torch::Tensor gamma,
    torch::Tensor beta,
    torch::Tensor running_mean,
    torch::Tensor running_var,
    float eps
) {
    auto batch_size = x.size(0);
    auto features = x.size(1);
    auto out = torch::empty_like(x);

    int total = batch_size * features;
    const int block_size = 256;
    int grid_size = (total + block_size - 1) / block_size;

    bn_gelu_relu_kernel<<<grid_size, block_size>>>(
        x.data_ptr<float>(),
        out.data_ptr<float>(),
        gamma.data_ptr<float>(),
        beta.data_ptr<float>(),
        running_mean.data_ptr<float>(),
        running_var.data_ptr<float>(),
        eps,
        batch_size,
        features
    );
    return out;
}
"""

cpp_src = r"""
torch::Tensor bn_gelu_relu_cuda(
    torch::Tensor x,
    torch::Tensor gamma,
    torch::Tensor beta,
    torch::Tensor running_mean,
    torch::Tensor running_var,
    float eps
);
"""

# Compile the fused BN + GeLU + ReLU CUDA extension
bn_gelu_relu = load_inline(
    name="bn_gelu_relu",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=["bn_gelu_relu_cuda"],
    verbose=False
)

class ModelNew(nn.Module):
    """
    Optimized model that uses PyTorch's Linear for GEMM, then applies a fused custom CUDA kernel
    for BatchNorm, GeLU, and ReLU.
    """
    def __init__(self, in_features, out_features):
        super(ModelNew, self).__init__()
        self.gemm = nn.Linear(in_features, out_features)
        self.bn_weight = nn.Parameter(torch.ones(out_features))
        self.bn_bias = nn.Parameter(torch.zeros(out_features))
        self.register_buffer("bn_running_mean", torch.zeros(out_features))
        self.register_buffer("bn_running_var", torch.ones(out_features))
        self.bn_eps = 1e-5

    def forward(self, x):
        """
        x: (batch_size, in_features)
        """
        # GEMM
        x = self.gemm(x)
        # Fused BatchNorm + GeLU + ReLU
        x = bn_gelu_relu.bn_gelu_relu_cuda(
            x,
            self.bn_weight,
            self.bn_bias,
            self.bn_running_mean,
            self.bn_running_var,
            self.bn_eps
        )
        return x

batch_size = 16384
in_features = 4096
out_features = 4096

def get_inputs():
    return [torch.rand(batch_size, in_features).cuda()]

def get_init_inputs():
    return [in_features, out_features]
```