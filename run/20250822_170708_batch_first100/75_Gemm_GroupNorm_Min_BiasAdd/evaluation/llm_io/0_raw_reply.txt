```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

# Naive fused and standalone CUDA kernels for demonstration purposes.
# In practice, you may wish to rely on more optimized libraries (e.g. cuBLAS) or
# advanced kernel fusion/tiling strategies for best performance.

source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <vector>
#include <cmath>

// ----------------------------------------
//  GEMM (matrix multiplication) kernel
//    x shape: [batch_size, in_features]
//    w shape: [out_features, in_features]
//    b shape: [out_features]
//    out shape: [batch_size, out_features]
// ----------------------------------------
__global__ void gemm_kernel(
    const float* __restrict__ x,
    const float* __restrict__ w,
    const float* __restrict__ b,
    float* __restrict__ out,
    int batch_size,
    int in_features,
    int out_features)
{
    // 2D grid of (row, col) -> (batch_idx, out_feature_idx)
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    if (row < batch_size && col < out_features) {
        float val = 0.0f;
        for (int k = 0; k < in_features; k++) {
            val += x[row * in_features + k] * w[col * in_features + k];
        }
        val += b[col];
        out[row * out_features + col] = val;
    }
}

torch::Tensor gemm_cuda(
    torch::Tensor x,
    torch::Tensor w,
    torch::Tensor b)
{
    // x: [batch_size, in_features]
    // w: [out_features, in_features]
    // b: [out_features]
    TORCH_CHECK(x.is_cuda(), "x must be a CUDA tensor");
    TORCH_CHECK(w.is_cuda(), "w must be a CUDA tensor");
    TORCH_CHECK(b.is_cuda(), "b must be a CUDA tensor");
    
    int batch_size = x.size(0);
    int in_features = x.size(1);
    int out_features = w.size(0);

    auto out = torch::empty({batch_size, out_features}, x.options());
    
    dim3 block(16, 16);
    dim3 grid((out_features + block.x - 1) / block.x,
              (batch_size + block.y - 1) / block.y);

    gemm_kernel<<<grid, block>>>(
        x.data_ptr<float>(),
        w.data_ptr<float>(),
        b.data_ptr<float>(),
        out.data_ptr<float>(),
        batch_size,
        in_features,
        out_features
    );

    return out;
}

// ----------------------------------------
//  GroupNorm kernel (2D: [N, C])
//    We assume N = batch_size, C = out_features
//    group_count = num_groups
//    Each group has group_size = C / group_count channels
//    For each sample, we compute mean/var per group, then normalize
//    We also apply optional gamma/beta from PyTorch GroupNorm
// ----------------------------------------

// Kernel pass #1: compute sums for mean/var
__global__ void groupnorm_sums_kernel(
    const float* __restrict__ x,
    float* __restrict__ sum_per_group,
    float* __restrict__ sumsq_per_group,
    int N,       // batch_size
    int C,       // channels
    int G,       // num_groups
    int group_size) // C / G
{
    // blockIdx.x -> group index
    // blockIdx.y -> sample index
    // we do a partial sum for channels in that group
    int group_idx = blockIdx.x;
    int n = blockIdx.y;
    int start_ch = group_idx * group_size;
    int end_ch = start_ch + group_size;

    float sum_val  = 0.0f;
    float sum_sq   = 0.0f;
    for (int c = start_ch; c < end_ch; c++) {
        float val = x[n * C + c];
        sum_val  += val;
        sum_sq   += val * val;
    }
    // Each block handles one (n, group_idx) => store partial sums
    sum_per_group[n * G + group_idx]   = sum_val;
    sumsq_per_group[n * G + group_idx] = sum_sq;
}

// Kernel pass #2: normalize using computed mean/var
__global__ void groupnorm_norm_kernel(
    const float* __restrict__ x,
    float* __restrict__ y,
    const float* __restrict__ sum_per_group,
    const float* __restrict__ sumsq_per_group,
    const float* __restrict__ gamma,
    const float* __restrict__ beta,
    int N,
    int C,
    int G,
    int group_size,
    float eps)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N * C) {
        int n = idx / C;
        int c = idx % C;
        int group_idx = c / group_size;
        float sum_val  = sum_per_group[n * G + group_idx];
        float sum_sq   = sumsq_per_group[n * G + group_idx];

        // mean/var per group
        float mean = sum_val / group_size;
        float var  = sum_sq / group_size - mean * mean;
        float inv_std = 1.0f / sqrtf(var + eps);

        // apply normalization
        float val  = x[idx];
        float g    = (gamma == nullptr) ? 1.0f : gamma[c];
        float b    = (beta  == nullptr) ? 0.0f : beta[c];
        float norm = (val - mean) * inv_std * g + b;
        y[idx] = norm;
    }
}

torch::Tensor group_norm_cuda(
    torch::Tensor x,      // [N, C]
    torch::Tensor gamma,  // [C]
    torch::Tensor beta,   // [C]
    int num_groups,
    double eps)
{
    TORCH_CHECK(x.dim() == 2, "Expected input of shape [N, C] for group norm demo");
    TORCH_CHECK(x.is_cuda(), "x must be CUDA");
    if (gamma.defined()) { TORCH_CHECK(gamma.is_cuda(), "gamma must be CUDA"); }
    if (beta.defined())  { TORCH_CHECK(beta.is_cuda(),  "beta must be CUDA"); }

    int N = x.size(0);
    int C = x.size(1);
    TORCH_CHECK(C % num_groups == 0, "C must be divisible by num_groups");
    int group_size = C / num_groups;

    auto y = torch::empty_like(x);
    // Temporary buffers for sums
    auto sum_per_group   = torch::zeros({N, num_groups}, x.options());
    auto sumsq_per_group = torch::zeros({N, num_groups}, x.options());

    dim3 block1(1, 1);
    dim3 grid1(num_groups, N);
    // compute sums
    groupnorm_sums_kernel<<<grid1, block1>>>(
        x.data_ptr<float>(),
        sum_per_group.data_ptr<float>(),
        sumsq_per_group.data_ptr<float>(),
        N, C, num_groups, group_size);

    // compute normalization
    int total = N * C;
    int block2 = 256;
    int grid2  = (total + block2 - 1) / block2;
    groupnorm_norm_kernel<<<grid2, block2>>>(
        x.data_ptr<float>(),
        y.data_ptr<float>(),
        sum_per_group.data_ptr<float>(),
        sumsq_per_group.data_ptr<float>(),
        gamma.defined() ? gamma.data_ptr<float>() : nullptr,
        beta.defined()  ? beta.data_ptr<float>()  : nullptr,
        N, C, num_groups, group_size, eps);

    return y;
}

// ----------------------------------------
//  Min across dim=1 kernel
//    in: [N, C]
//    out: [N, 1]
// ----------------------------------------
__global__ void min_dim1_kernel(
    const float* __restrict__ x,
    float* __restrict__ out,
    int N,
    int C)
{
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    if (row < N) {
        float mval = x[row * C];
        for (int c = 1; c < C; c++) {
            float v = x[row * C + c];
            if (v < mval) {
                mval = v;
            }
        }
        // store result as single column
        out[row] = mval;
    }
}

torch::Tensor min_dim1_cuda(torch::Tensor x)
{
    TORCH_CHECK(x.dim() == 2, "min_dim1 expects [N, C]");
    TORCH_CHECK(x.is_cuda(), "x must be CUDA");
    int N = x.size(0);
    int C = x.size(1);

    // output shape: [N, 1]
    auto out = torch::empty({N, 1}, x.options());

    int block = 256;
    int grid  = (N + block - 1) / block;
    min_dim1_kernel<<<grid, block>>>(
        x.data_ptr<float>(),
        out.data_ptr<float>(),
        N, C);

    return out;
}

// ----------------------------------------
//  Add bias kernel, broadcast from [1, C, 1, 1] to [N, C, 1, 1]
//  However, for demonstration, we treat out as [N, C] after expansion
//  and bias as a flattened [C]. If the user needs a different shape
//  broadcast, please adapt accordingly.
// ----------------------------------------
__global__ void add_bias_kernel(
    const float* __restrict__ x,
    const float* __restrict__ bias,
    float* __restrict__ out,
    int N,
    int C)
{
    // 2D indexing
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    if (row < N && col < C) {
        // x is [N, 1] if we came from min, but we want [N, C].
        // We'll broadcast x[row, 0] over all columns, and
        // add bias[col].
        float val = x[row];
        out[row * C + col] = val + bias[col];
    }
}

torch::Tensor add_bias_cuda(torch::Tensor x, torch::Tensor bias)
{
    // x: [N, 1] (result of min); we broadcast across C
    // bias: [1, C, 1, 1], effectively [C] after flatten
    TORCH_CHECK(x.is_cuda(), "x must be CUDA");
    TORCH_CHECK(bias.is_cuda(), "bias must be CUDA");
    int N = x.size(0);
    // flatten bias to [C]
    int C = bias.numel();
    auto out = torch::empty({N, C}, x.options());

    dim3 block(16, 16);
    dim3 grid((C + block.x - 1) / block.x,
              (N + block.y - 1) / block.y);

    add_bias_kernel<<<grid, block>>>(
        x.data_ptr<float>(),
        bias.data_ptr<float>(),
        out.data_ptr<float>(),
        N, C);

    return out;
}
""";

# Prototypes for the above kernels
cpp_src = r"""
torch::Tensor gemm_cuda(torch::Tensor x, torch::Tensor w, torch::Tensor b);
torch::Tensor group_norm_cuda(torch::Tensor x, torch::Tensor gamma, torch::Tensor beta, int num_groups, double eps);
torch::Tensor min_dim1_cuda(torch::Tensor x);
torch::Tensor add_bias_cuda(torch::Tensor x, torch::Tensor bias);
"""

# Build the inline extension
model_ops = load_inline(
    name="model_ops",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=["gemm_cuda", "group_norm_cuda", "min_dim1_cuda", "add_bias_cuda"],
    verbose=False,
)

class ModelNew(nn.Module):
    """
    Optimized model with custom CUDA kernels replacing PyTorch ops:
      1) GEMM with naive custom kernel
      2) GroupNorm with naive custom kernel
      3) Min across dim=1 with custom kernel
      4) Add bias with broadcast in custom kernel
    """
    def __init__(self, in_features, out_features, num_groups, bias_shape):
        super(ModelNew, self).__init__()
        # These modules are used only to store and track the official parameters
        self.gemm = nn.Linear(in_features, out_features, bias=True)
        self.group_norm = nn.GroupNorm(num_groups, out_features)
        # We store a "bias" parameter of shape bias_shape
        self.bias = nn.Parameter(torch.randn(bias_shape))

        self.num_groups = num_groups

    def forward(self, x):
        # 1) GEMM
        #   (weight.shape = [out_features, in_features], bias.shape = [out_features])
        x = model_ops.gemm_cuda(
            x,
            self.gemm.weight,
            self.gemm.bias
        )

        # 2) GroupNorm (expects shape [N, C])
        #    group_norm has learned weight/bias in self.group_norm.weight / self.group_norm.bias
        #    if one desires them, pass them as gamma/beta
        gamma = self.group_norm.weight if self.group_norm.weight is not None else torch.empty(0)
        beta  = self.group_norm.bias  if self.group_norm.bias  is not None else torch.empty(0)
        x = model_ops.group_norm_cuda(
            x,
            gamma,
            beta,
            self.num_groups,
            1e-5
        )

        # 3) Min across dim=1 (returns shape [N, 1])
        x = model_ops.min_dim1_cuda(x)

        # 4) Add the learnable bias with broadcast
        #    bias is shape [1, out_features, 1, 1], effectively [out_features] after flatten
        x = model_ops.add_bias_cuda(
            x,
            self.bias.view(-1)  # flatten the bias
        )
        return x
```
