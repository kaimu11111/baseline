```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__device__ inline float gelu(float x) {
    // Approximate or exact version. Here we use the error function-based approach:
    return 0.5f * x * (1.0f + erff(x / M_SQRT2));
}

__global__ void fused_activations_kernel(const float* __restrict__ input,
                                         const float* __restrict__ bias,
                                         float* __restrict__ output,
                                         const int batch_size,
                                         const int out_channels,
                                         const int depth,
                                         const int height,
                                         const int width,
                                         const float negative_slope) 
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total_size = batch_size * out_channels * depth * height * width;
    if (idx >= total_size) return;

    // Decompose idx into (b, c, d, h, w)
    int w_ = idx % width;
    int h_ = (idx / width) % height;
    int d_ = (idx / (width * height)) % depth;
    int c_ = (idx / (width * height * depth)) % out_channels;
    // int b_ = idx / (width * height * depth * out_channels);

    float val = input[idx];

    // 1) ReLU
    val = fmaxf(0.0f, val);

    // 2) LeakyReLU
    // Since ReLU may zero out negatives, this is effectively max(0, val) 
    // but we still include the formula:
    val = (val >= 0.0f) ? val : negative_slope * val;

    // 3) GELU
    val = gelu(val);

    // 4) Sigmoid
    val = 1.0f / (1.0f + expf(-val));

    // 5) Add bias (broadcast on channel dim)
    val += bias[c_];

    output[idx] = val;
}

torch::Tensor fused_activations_cuda(torch::Tensor x,
                                     torch::Tensor bias,
                                     float negative_slope) 
{
    auto sizes = x.sizes();
    int batch_size = sizes[0];
    int out_channels = sizes[1];
    int depth = sizes[2];
    int height = sizes[3];
    int width = sizes[4];

    auto output = torch::empty_like(x);

    int total_size = batch_size * out_channels * depth * height * width;
    const int block_size = 256;
    const int grid_size = (total_size + block_size - 1) / block_size;

    fused_activations_kernel<<<grid_size, block_size>>>(x.data_ptr<float>(),
                                                        bias.data_ptr<float>(),
                                                        output.data_ptr<float>(),
                                                        batch_size,
                                                        out_channels,
                                                        depth,
                                                        height,
                                                        width,
                                                        negative_slope);

    return output;
}
"""

cpp_src = """
torch::Tensor fused_activations_cuda(torch::Tensor x,
                                     torch::Tensor bias,
                                     float negative_slope);
"""

fused_activations = load_inline(
    name="fused_activations",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=["fused_activations_cuda"],
    verbose=False
)

class ModelNew(nn.Module):
    """
    Model that performs a 3D convolution, then fuses ReLU, LeakyReLU, GELU, Sigmoid, and bias addition into one CUDA kernel.
    """
    def __init__(self, in_channels, out_channels, kernel_size, bias_shape):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size)
        self.bias = nn.Parameter(torch.randn(bias_shape))
        self.negative_slope = 0.01  # for LeakyReLU

    def forward(self, x):
        x = self.conv(x)
        # Fused activation + bias
        x = fused_activations.fused_activations_cuda(x, self.bias, self.negative_slope)
        return x

def get_inputs():
    batch_size = 64
    in_channels = 8
    depth, height, width = 32, 64, 64
    return [torch.rand(batch_size, in_channels, depth, height, width)]

def get_init_inputs():
    in_channels = 8
    out_channels = 32
    kernel_size = 3
    bias_shape = (out_channels, 1, 1, 1)
    return [in_channels, out_channels, kernel_size, bias_shape]
```