```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom CUDA source for fused GEMM + scale
source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>

__global__ void fused_gemm_scale_kernel(
    const float* __restrict__ x,
    const float* __restrict__ w,
    const float* __restrict__ bias,
    const float* __restrict__ scale,
    float* __restrict__ out,
    int M, int K, int N
) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < M && col < N) {
        float val = 0.0f;
        for (int i = 0; i < K; i++) {
            val += x[row * K + i] * w[col * K + i];
        }
        val += bias[col];
        val *= scale[col];
        out[row * N + col] = val;
    }
}

torch::Tensor fused_gemm_scale_cuda(
    torch::Tensor x,
    torch::Tensor w,
    torch::Tensor bias,
    torch::Tensor scale
) {
    const auto M = x.size(0);
    const auto K = x.size(1);
    const auto N = w.size(0);

    auto out = torch::empty({M, N}, x.options());

    dim3 block(16, 16);
    dim3 grid((N + block.x - 1) / block.x, (M + block.y - 1) / block.y);

    fused_gemm_scale_kernel<<<grid, block>>>(
        x.data_ptr<float>(),
        w.data_ptr<float>(),
        bias.data_ptr<float>(),
        scale.data_ptr<float>(),
        out.data_ptr<float>(),
        M, K, N
    );

    return out;
}
""";

# Corresponding C++ function declaration
cpp_src = r"""
torch::Tensor fused_gemm_scale_cuda(
    torch::Tensor x,
    torch::Tensor w,
    torch::Tensor bias,
    torch::Tensor scale
);
""";

# Build and load the fused operator
fused_ops = load_inline(
    name="fused_gemm_scale",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=["fused_gemm_scale_cuda"],
    verbose=False
)

class ModelNew(nn.Module):
    """
    Optimized model: fused GEMM + scale in a custom CUDA kernel, followed by PyTorch batch normalization.
    """
    def __init__(self, in_features, out_features, scale_shape, eps=1e-5, momentum=0.1):
        super(ModelNew, self).__init__()
        # Replace nn.Linear with custom weight/bias parameters
        self.weight = nn.Parameter(torch.randn(out_features, in_features, device='cuda'))
        self.bias = nn.Parameter(torch.zeros(out_features, device='cuda'))
        self.scale = nn.Parameter(torch.randn(scale_shape, device='cuda'))
        self.bn = nn.BatchNorm1d(out_features, eps=eps, momentum=momentum).cuda()

    def forward(self, x):
        # Fused: (x * weight^T + bias) * scale
        x = fused_ops.fused_gemm_scale_cuda(x, self.weight, self.bias, self.scale)
        # Apply batch normalization
        x = self.bn(x)
        return x
```