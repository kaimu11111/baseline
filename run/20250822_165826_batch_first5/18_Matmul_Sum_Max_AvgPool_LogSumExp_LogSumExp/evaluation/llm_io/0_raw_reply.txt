```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

batch_size = 1024
in_features = 8192
out_features = 8192

# Inline CUDA source for custom rowwise reductions
# We provide kernels for sum, max, mean, and logsumexp across dim=1 (rowwise).
source = r'''
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <float.h>

// Each block processes one row. We use a blockDim.x of up to 1024 threads (or 256, etc.)
// to do a partial reduction within shared memory. Then we store the result into out[row].

template <typename Func>
__device__ float block_reduce(float val, Func red_fun) {
    // Warp-wise reduction
    val = red_fun(val, __shfl_down_sync(0xffffffff, val, 16));
    val = red_fun(val, __shfl_down_sync(0xffffffff, val, 8));
    val = red_fun(val, __shfl_down_sync(0xffffffff, val, 4));
    val = red_fun(val, __shfl_down_sync(0xffffffff, val, 2));
    val = red_fun(val, __shfl_down_sync(0xffffffff, val, 1));
    return val;
}

__global__ void rowwise_sum_kernel(const float* __restrict__ x, float* __restrict__ out,
                                   const int rows, const int cols) {
    int row = blockIdx.x;
    if (row >= rows) return;

    float thread_sum = 0.0f;
    // Each thread processes some of the elements in that row
    for (int col = threadIdx.x; col < cols; col += blockDim.x) {
        thread_sum += x[row * cols + col];
    }

    // Warp reduction
    // First reduce within warp
    float warp_sum = block_reduce(thread_sum, [](float a, float b){ return a + b; });

    // Each warp's 0th lane writes to shared memory then we reduce
    static __shared__ float smem[32]; // up to 32 warps in block
    int warp_id = threadIdx.x / 32;

    if ((threadIdx.x & 31) == 0) {
        smem[warp_id] = warp_sum;
    }
    __syncthreads();

    // Reduce warp sums
    if (warp_id == 0) {
        float val = (threadIdx.x < (blockDim.x / 32)) ? smem[threadIdx.x] : 0.0f;
        float final_sum = block_reduce(val, [](float a, float b){ return a + b; });
        if (threadIdx.x == 0) {
            out[row] = final_sum;
        }
    }
}

__global__ void rowwise_max_kernel(const float* __restrict__ x, float* __restrict__ out,
                                   const int rows, const int cols) {
    int row = blockIdx.x;
    if (row >= rows) return;

    float thread_max = -FLT_MAX;
    for (int col = threadIdx.x; col < cols; col += blockDim.x) {
        float val = x[row * cols + col];
        if (val > thread_max) thread_max = val;
    }

    // Warp reduction for max
    float warp_max = block_reduce(thread_max, [](float a, float b){ return a > b ? a : b; });

    static __shared__ float smem[32];
    int warp_id = threadIdx.x / 32;
    if ((threadIdx.x & 31) == 0) {
        smem[warp_id] = warp_max;
    }
    __syncthreads();

    if (warp_id == 0) {
        float val = (threadIdx.x < (blockDim.x / 32)) ? smem[threadIdx.x] : -FLT_MAX;
        float final_max = block_reduce(val, [](float a, float b){ return a > b ? a : b; });
        if (threadIdx.x == 0) {
            out[row] = final_max;
        }
    }
}

__global__ void rowwise_mean_kernel(const float* __restrict__ x, float* __restrict__ out,
                                    const int rows, const int cols) {
    int row = blockIdx.x;
    if (row >= rows) return;

    float thread_sum = 0.0f;
    for (int col = threadIdx.x; col < cols; col += blockDim.x) {
        thread_sum += x[row * cols + col];
    }

    // Warp reduction for sum
    float warp_sum = block_reduce(thread_sum, [](float a, float b){ return a + b; });

    static __shared__ float smem[32];
    int warp_id = threadIdx.x / 32;
    if ((threadIdx.x & 31) == 0) {
        smem[warp_id] = warp_sum;
    }
    __syncthreads();

    if (warp_id == 0) {
        float val = (threadIdx.x < (blockDim.x / 32)) ? smem[threadIdx.x] : 0.0f;
        float final_sum = block_reduce(val, [](float a, float b){ return a + b; });
        if (threadIdx.x == 0) {
            out[row] = final_sum / (float)cols;
        }
    }
}

__global__ void rowwise_logsumexp_kernel(const float* __restrict__ x, float* __restrict__ out,
                                         const int rows, const int cols) {
    int row = blockIdx.x;
    if (row >= rows) return;

    // Find max
    float thread_max = -FLT_MAX;
    for (int col = threadIdx.x; col < cols; col += blockDim.x) {
        float val = x[row * cols + col];
        thread_max = (val > thread_max) ? val : thread_max;
    }
    float warp_max = block_reduce(thread_max, [](float a, float b){ return a > b ? a : b; });

    static __shared__ float max_smem[32];
    int warp_id = threadIdx.x / 32;
    if ((threadIdx.x & 31) == 0) {
        max_smem[warp_id] = warp_max;
    }
    __syncthreads();

    float row_max = -FLT_MAX;
    if (warp_id == 0) {
        float val = (threadIdx.x < (blockDim.x / 32)) ? max_smem[threadIdx.x] : -FLT_MAX;
        float final_max = block_reduce(val, [](float a, float b){ return a > b ? a : b; });
        if (threadIdx.x == 0) {
            row_max = final_max;
            max_smem[0] = final_max;
        }
    }
    __syncthreads();
    row_max = max_smem[0];

    // Now compute exp and sum
    float thread_sum = 0.0f;
    for (int col = threadIdx.x; col < cols; col += blockDim.x) {
        float val = x[row * cols + col];
        thread_sum += expf(val - row_max);
    }
    float warp_sum = block_reduce(thread_sum, [](float a, float b){ return a + b; });

    static __shared__ float sum_smem[32];
    if ((threadIdx.x & 31) == 0) {
        sum_smem[warp_id] = warp_sum;
    }
    __syncthreads();

    if (warp_id == 0) {
        float val = (threadIdx.x < (blockDim.x / 32)) ? sum_smem[threadIdx.x] : 0.0f;
        float final_sum = block_reduce(val, [](float a, float b){ return a + b; });
        if (threadIdx.x == 0) {
            out[row] = logf(final_sum) + row_max;
        }
    }
}

// C++ wrappers
torch::Tensor rowwise_sum_cuda(torch::Tensor x) {
    // x is 2D: (rows, cols)
    int rows = x.size(0);
    int cols = x.size(1);
    auto out = torch::zeros({rows}, x.options());
    int block_size = 256;
    dim3 grid(rows);
    dim3 block(block_size);
    rowwise_sum_kernel<<<grid, block>>>(x.data_ptr<float>(), out.data_ptr<float>(), rows, cols);
    return out;
}

torch::Tensor rowwise_max_cuda(torch::Tensor x) {
    int rows = x.size(0);
    int cols = x.size(1);
    auto out = torch::zeros({rows}, x.options());
    int block_size = 256;
    dim3 grid(rows);
    dim3 block(block_size);
    rowwise_max_kernel<<<grid, block>>>(x.data_ptr<float>(), out.data_ptr<float>(), rows, cols);
    return out;
}

torch::Tensor rowwise_mean_cuda(torch::Tensor x) {
    int rows = x.size(0);
    int cols = x.size(1);
    auto out = torch::zeros({rows}, x.options());
    int block_size = 256;
    dim3 grid(rows);
    dim3 block(block_size);
    rowwise_mean_kernel<<<grid, block>>>(x.data_ptr<float>(), out.data_ptr<float>(), rows, cols);
    return out;
}

torch::Tensor rowwise_logsumexp_cuda(torch::Tensor x) {
    int rows = x.size(0);
    int cols = x.size(1);
    auto out = torch::zeros({rows}, x.options());
    int block_size = 256;
    dim3 grid(rows);
    dim3 block(block_size);
    rowwise_logsumexp_kernel<<<grid, block>>>(x.data_ptr<float>(), out.data_ptr<float>(), rows, cols);
    return out;
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("rowwise_sum_cuda", &rowwise_sum_cuda, "Rowwise Sum");
    m.def("rowwise_max_cuda", &rowwise_max_cuda, "Rowwise Max");
    m.def("rowwise_mean_cuda", &rowwise_mean_cuda, "Rowwise Mean");
    m.def("rowwise_logsumexp_cuda", &rowwise_logsumexp_cuda, "Rowwise LogSumExp");
}
'''

# Declarations for the C++ calls
cpp_src = r'''
torch::Tensor rowwise_sum_cuda(torch::Tensor x);
torch::Tensor rowwise_max_cuda(torch::Tensor x);
torch::Tensor rowwise_mean_cuda(torch::Tensor x);
torch::Tensor rowwise_logsumexp_cuda(torch::Tensor x);
'''

# Load the inline CUDA extension
rowwise_ops = load_inline(
    name="rowwise_ops",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=[
        "rowwise_sum_cuda",
        "rowwise_max_cuda",
        "rowwise_mean_cuda",
        "rowwise_logsumexp_cuda",
    ],
    verbose=False,
)


class ModelNew(nn.Module):
    """
    Optimized model using custom CUDA kernels for sum, max, mean, and logsumexp.
    """
    def __init__(self, in_features, out_features):
        super(ModelNew, self).__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.ops = rowwise_ops

    def forward(self, x):
        # Same sequence of ops, replaced with custom kernels
        x = self.linear(x)  # (batch_size, out_features)

        # Sum across dim=1
        x = self.ops.rowwise_sum_cuda(x)
        x = x.unsqueeze(1)  # shape (batch_size, 1)

        # Max across dim=1
        x = self.ops.rowwise_max_cuda(x)
        x = x.unsqueeze(1)  # shape (batch_size, 1)

        # Mean across dim=1
        x = self.ops.rowwise_mean_cuda(x)
        x = x.unsqueeze(1)  # shape (batch_size, 1)

        # LogSumExp across dim=1
        x = self.ops.rowwise_logsumexp_cuda(x)
        x = x.unsqueeze(1)  # shape (batch_size, 1)

        # LogSumExp again across dim=1
        x = self.ops.rowwise_logsumexp_cuda(x)
        x = x.unsqueeze(1)  # shape (batch_size, 1)

        return x

def get_inputs():
    return [torch.rand(batch_size, in_features).cuda()]

def get_init_inputs():
    return [in_features, out_features]
```