```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# Custom fused kernel for Mish + add_value + Hardtanh + scale
source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

__global__ void fused_ops_kernel(const float* __restrict__ input,
                                 float* __restrict__ output,
                                 const float add_value,
                                 const float scale,
                                 const int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        float x = input[idx];
        // Mish: x * tanh(softplus(x))
        float sp = logf(1.0f + expf(x)); 
        float mish_val = x * tanhf(sp);
        // add_value
        mish_val += add_value;
        // hardtanh between -1 and 1
        mish_val = fmaxf(-1.0f, fminf(1.0f, mish_val));
        // scale
        mish_val *= scale;
        output[idx] = mish_val;
    }
}

torch::Tensor fused_ops_cuda(torch::Tensor input, float add_value, float scale) {
    auto out = torch::empty_like(input);
    int size = input.numel();
    const int block_size = 256;
    const int grid_size = (size + block_size - 1) / block_size;

    fused_ops_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        out.data_ptr<float>(),
        add_value,
        scale,
        size
    );
    return out;
}
"""

cpp_src = r"""
torch::Tensor fused_ops_cuda(torch::Tensor input, float add_value, float scale);
"""

fused_ops = load_inline(
    name="fused_ops",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=["fused_ops_cuda"],
    verbose=False,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    """
    Optimized Model that performs a transposed convolution, then uses a fused CUDA kernel
    to apply Mish activation, adds a value, applies Hardtanh activation, then scales the output.
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, add_value, scale):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding)
        self.add_value = add_value
        self.scale = scale
        self.fused_ops = fused_ops

    def forward(self, x):
        x = self.conv_transpose(x)
        x = x.contiguous()  # ensure contiguous memory for kernel
        x = self.fused_ops.fused_ops_cuda(x, self.add_value, self.scale)
        return x
```