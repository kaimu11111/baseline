```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline

source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

// Kernel for computing sum and sum of squares per [n,c] across H*W (naive reduction).
__global__ void compute_sum_sumsq_kernel(const float* __restrict__ x,
                                         float* __restrict__ sum,
                                         float* __restrict__ sumsq,
                                         int N, int C, int H, int W) {
    int nc = blockIdx.x;
    if (nc >= N*C) return;

    int n = nc / C;
    int c = nc % C;
    float thread_sum = 0.0f;
    float thread_sumsq = 0.0f;

    for (int idx = threadIdx.x; idx < H*W; idx += blockDim.x) {
        int h = idx / W;
        int w = idx % W;
        int input_idx = ((n*C + c)*H + h)*W + w;
        float val = x[input_idx];
        thread_sum += val;
        thread_sumsq += val * val;
    }

    __shared__ float shared_sum[256];
    __shared__ float shared_sumsq[256];
    shared_sum[threadIdx.x] = thread_sum;
    shared_sumsq[threadIdx.x] = thread_sumsq;
    __syncthreads();

    // Parallel reduction within each block
    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
        if (threadIdx.x < stride) {
            shared_sum[threadIdx.x] += shared_sum[threadIdx.x + stride];
            shared_sumsq[threadIdx.x] += shared_sumsq[threadIdx.x + stride];
        }
        __syncthreads();
    }

    // Accumulate into global sum
    if (threadIdx.x == 0) {
        atomicAdd(&sum[nc], shared_sum[0]);
        atomicAdd(&sumsq[nc], shared_sumsq[0]);
    }
}

// Kernel for applying InstanceNorm + division
__global__ void apply_instnorm_div_kernel(const float* __restrict__ x,
                                          float* __restrict__ out,
                                          const float* __restrict__ mean,
                                          const float* __restrict__ var,
                                          const float* __restrict__ weight,
                                          const float* __restrict__ bias,
                                          float eps, float div_val,
                                          int N, int C, int H, int W) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total_size = N*C*H*W;
    if (idx >= total_size) return;

    int w = idx % W;
    int h = (idx / W) % H;
    int c = (idx / (W*H)) % C;
    int n = idx / (W*H*C);

    int nc = n*C + c;
    float m = mean[nc];
    float v = var[nc];
    float inv_std = rsqrtf(v + eps);

    float val = x[idx];
    // Instance norm transform
    val = (val - m) * inv_std;
    // Affine
    val = val * weight[c] + bias[c];
    // Division
    val /= div_val;

    out[idx] = val;
}

torch::Tensor fused_instance_norm_div(
    torch::Tensor x,
    torch::Tensor weight,
    torch::Tensor bias,
    float eps,
    float div_val
) {
    // x: [N, C, H, W]
    int N = x.size(0);
    int C = x.size(1);
    int H = x.size(2);
    int W = x.size(3);

    auto out = torch::empty_like(x);

    // Prepare buffers for sum and sum of squares
    auto sum = torch::zeros({N*C}, x.options());
    auto sumsq = torch::zeros({N*C}, x.options());

    // Launch kernel to compute sums (per [n,c])
    dim3 grid(N*C);
    dim3 block(256);
    compute_sum_sumsq_kernel<<<grid, block>>>(
        x.data_ptr<float>(),
        sum.data_ptr<float>(),
        sumsq.data_ptr<float>(),
        N, C, H, W
    );

    // Compute mean & var on CPU (for demonstration)
    auto sum_cpu = sum.cpu();
    auto sumsq_cpu = sumsq.cpu();
    auto mean = torch::zeros({N*C}, x.options());
    auto var = torch::zeros({N*C}, x.options());
    auto mean_cpu = torch::zeros({N*C}, sum_cpu.options().device(torch::kCPU));
    auto var_cpu = torch::zeros({N*C}, sum_cpu.options().device(torch::kCPU));

    float* sum_ptr = (float*)sum_cpu.data_ptr();
    float* sumsq_ptr = (float*)sumsq_cpu.data_ptr();
    float* mean_ptr = (float*)mean_cpu.data_ptr();
    float* var_ptr = (float*)var_cpu.data_ptr();

    int area = H * W;
    for (int i = 0; i < N*C; i++) {
        float m = sum_ptr[i] / area;
        float v = sumsq_ptr[i] / area - m*m;
        mean_ptr[i] = m;
        var_ptr[i] = v;
    }

    // Copy mean/var back to GPU
    mean.copy_(mean_cpu);
    var.copy_(var_cpu);

    // Apply normalization + division
    int total_size = N*C*H*W;
    dim3 grid2((total_size + 255)/256);
    dim3 block2(256);
    apply_instnorm_div_kernel<<<grid2, block2>>>(
        x.data_ptr<float>(),
        out.data_ptr<float>(),
        mean.data_ptr<float>(),
        var.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias.data_ptr<float>(),
        eps,
        div_val,
        N, C, H, W
    );

    return out;
}
""";

cpp_src = r"""
torch::Tensor fused_instance_norm_div(
    torch::Tensor x,
    torch::Tensor weight,
    torch::Tensor bias,
    float eps,
    float div_val
);
""";

# Build the custom fused instance norm + division operator
fused_instnorm_div = load_inline(
    name="fused_instnorm_div",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=["fused_instance_norm_div"],
    verbose=False,
    extra_cflags=[""],
    extra_ldflags=[""],
)

class ModelNew(nn.Module):
    """
    Optimized model that uses custom CUDA operators for Instance Normalization + division.
    """
    def __init__(self, in_channels, out_channels, kernel_size, divide_by):
        super(ModelNew, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)
        self.instnorm = nn.InstanceNorm2d(out_channels, affine=True, track_running_stats=False)
        self.divide_by = divide_by

    def forward(self, x):
        # Keep convolution in PyTorch, fuse InstanceNorm and division into one CUDA kernel
        x = self.conv(x)
        x = fused_instnorm_div.fused_instance_norm_div(
            x, 
            self.instnorm.weight, 
            self.instnorm.bias, 
            self.instnorm.eps, 
            self.divide_by
        )
        return x

batch_size = 128
in_channels = 64
out_channels = 128
height = width = 128
kernel_size = 3
divide_by = 2.0

def get_inputs():
    return [torch.rand(batch_size, in_channels, height, width).cuda()]

def get_init_inputs():
    return [in_channels, out_channels, kernel_size, divide_by]
```