```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <math.h>

// Fused BatchNorm + Tanh kernel
__global__ void bn_tanh_kernel(
    const float* __restrict__ x,
    float* __restrict__ y,
    const float* __restrict__ mean,
    const float* __restrict__ var,
    const float* __restrict__ weight,
    const float* __restrict__ bias,
    const float eps,
    const int N,
    const int C,
    const int H,
    const int W
){
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total = N * C * H * W;
    if (idx < total) {
        int w = idx % W;
        int h = (idx / W) % H;
        int c = (idx / (W * H)) % C;
        // int n = idx / (W * H * C); // not needed if we only do channel-based normalization

        float val   = x[idx];
        float m     = mean[c];
        float v     = var[c];
        float w_c   = weight[c];
        float b_c   = bias[c];
        
        // batch-norm
        float normed = (val - m) / sqrtf(v + eps);
        normed = normed * w_c + b_c;
        // tanh
        y[idx] = tanhf(normed);
    }
}

torch::Tensor bn_tanh_cuda(
    torch::Tensor x,
    torch::Tensor mean,
    torch::Tensor var,
    torch::Tensor weight,
    torch::Tensor bias,
    float eps
){
    const int N = x.size(0);
    const int C = x.size(1);
    const int H = x.size(2);
    const int W = x.size(3);
    auto y = torch::empty_like(x);

    int total = N * C * H * W;
    int threads = 256;
    int blocks  = (total + threads - 1) / threads;

    bn_tanh_kernel<<<blocks, threads>>>(
        x.data_ptr<float>(),
        y.data_ptr<float>(),
        mean.data_ptr<float>(),
        var.data_ptr<float>(),
        weight.data_ptr<float>(),
        bias.data_ptr<float>(),
        eps,
        N, C, H, W
    );

    return y;
}

// 2x2 MaxPool2d with stride=2, assumes divisible H & W
__global__ void maxpool2d_2x2_kernel(
    const float* __restrict__ x,
    float* __restrict__ y,
    const int N,
    const int C,
    const int H,
    const int W
){
    // output shape: (N, C, H/2, W/2)
    int H_out = H >> 1;
    int W_out = W >> 1;
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total_out = N * C * H_out * W_out;

    if (idx < total_out) {
        int w_out = idx % W_out;
        int h_out = (idx / W_out) % H_out;
        int c     = (idx / (H_out * W_out)) % C;
        int n     = idx / (H_out * W_out * C);

        float max_val = -3.402823466e+38F; // float min
        for(int i = 0; i < 2; i++){
            for(int j = 0; j < 2; j++){
                int h_in = 2 * h_out + i;
                int w_in = 2 * w_out + j;
                int in_idx = n*C*H*W + c*H*W + h_in*W + w_in;
                float val = x[in_idx];
                max_val = (val > max_val) ? val : max_val;
            }
        }
        y[idx] = max_val;
    }
}

torch::Tensor maxpool2d_2x2_cuda(torch::Tensor x){
    const int N = x.size(0);
    const int C = x.size(1);
    const int H = x.size(2);
    const int W = x.size(3);

    int H_out = H >> 1;
    int W_out = W >> 1;

    auto y = torch::empty({N, C, H_out, W_out}, x.options());

    int total_out = N * C * H_out * W_out;
    int threads = 256;
    int blocks  = (total_out + threads - 1) / threads;

    maxpool2d_2x2_kernel<<<blocks, threads>>>(
        x.data_ptr<float>(),
        y.data_ptr<float>(),
        N, C, H, W
    );
    return y;
}
"""

cpp_src = r"""
torch::Tensor bn_tanh_cuda(
    torch::Tensor x,
    torch::Tensor mean,
    torch::Tensor var,
    torch::Tensor weight,
    torch::Tensor bias,
    float eps
);

torch::Tensor maxpool2d_2x2_cuda(torch::Tensor x);
"""

fused_ops = load_inline(
    name="fused_ops",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=["bn_tanh_cuda", "maxpool2d_2x2_cuda"],
    verbose=False
)

class ModelNew(nn.Module):
    """
    Optimized model that fuses BatchNorm + Tanh into a single custom kernel,
    and uses a custom 2x2 MaxPool2d kernel with stride=2.
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups, num_groups):
        super(ModelNew, self).__init__()
        # Keep conv transpose as-is
        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding)
        # We'll store the BN layer to access the parameters, but won't call it in forward
        self.batch_norm = nn.BatchNorm2d(out_channels)
        # Keep group norm as-is
        self.group_norm = nn.GroupNorm(num_groups=num_groups, num_channels=out_channels)

    def forward(self, x):
        # regular transposed convolution
        x = self.conv_transpose(x)
        # fused BN + Tanh via custom kernel
        x = fused_ops.bn_tanh_cuda(
            x,
            self.batch_norm.running_mean,
            self.batch_norm.running_var,
            self.batch_norm.weight,
            self.batch_norm.bias,
            self.batch_norm.eps
        )
        # custom 2x2 max pool
        x = fused_ops.maxpool2d_2x2_cuda(x)
        # group normalization
        x = self.group_norm(x)
        return x
```
