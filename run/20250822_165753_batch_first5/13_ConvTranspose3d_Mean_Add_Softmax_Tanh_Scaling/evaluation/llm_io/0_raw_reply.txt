```python
import torch
import torch.nn as nn
from torch.utils.cpp_extension import load_inline

# --------------------------------------------------------
# Inline CUDA/C++ code for custom kernels
# --------------------------------------------------------
source = r"""
#include <torch/extension.h>
#include <cuda_runtime.h>
#include <vector>

// ---------------------------------------
// KERNEL 1: 3D mean pooling over depth
// (B, C, D, H, W) --> (B, C, 1, H, W)
// ---------------------------------------
__global__ void mean_pool_3d_kernel(const float* __restrict__ input,
                                    float* __restrict__ output,
                                    const int B, const int C,
                                    const int D, const int H, const int W) {
    // Grid dimensions:
    //   gridDim.x = B
    //   gridDim.y = C
    //   gridDim.z = number of needed blocks to cover H*W
    // Thread dimension: blockDim.x = block_size
    int b = blockIdx.x;
    int c = blockIdx.y;
    int hw_index = blockIdx.z * blockDim.x + threadIdx.x;
    if (hw_index < H * W) {
        int h = hw_index / W;
        int w = hw_index % W;

        // Accumulate sum over D dimension
        float sum_val = 0.0f;
        int base_in = b * C * D * H * W + c * D * H * W;
        for(int d = 0; d < D; d++){
            sum_val += input[base_in + d * H * W + h * W + w];
        }
        sum_val /= D; // mean
        int out_index = b * C * H * W + c * H * W + h * W + w;
        output[out_index] = sum_val;
    }
}

torch::Tensor mean_pool_3d_cuda(torch::Tensor input) {
    TORCH_CHECK(input.is_cuda(), "Input must be a CUDA tensor");

    const auto B = input.size(0);
    const auto C = input.size(1);
    const auto D = input.size(2);
    const auto H = input.size(3);
    const auto W = input.size(4);

    // Output shape: (B, C, 1, H, W)
    auto out_options = input.options();
    auto out = torch::zeros({B, C, H, W}, out_options);

    const int block_size = 256;
    const int total_hw = H * W;
    dim3 grid(B, C, (total_hw + block_size - 1) / block_size);
    dim3 block(block_size);

    mean_pool_3d_kernel<<<grid, block>>>(input.data_ptr<float>(),
                                         out.data_ptr<float>(),
                                         B, C, D, H, W);
    return out.view({B, C, 1, H, W});
}

// ---------------------------------------------------
// KERNEL 2: Bias + Softmax + Tanh + Scale (dim=1)
// Input shape: (B, C, 1, H, W)
// ---------------------------------------------------
__global__ void add_bias_softmax_tanh_scale_kernel(const float* __restrict__ input,
                                                   const float* __restrict__ bias,
                                                   float* __restrict__ output,
                                                   const int B, const int C,
                                                   const int H, const int W,
                                                   const float scale) {
    // We map each (B, H, W) to a single thread, then loop over channels
    //   gridDim.x * blockDim.x covers B*H*W
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    int total_bhw = B * H * W;
    if (idx >= total_bhw) return;

    // decode b, h, w from idx
    int b = idx / (H * W);
    int hw = idx % (H * W);
    int h = hw / W;
    int w = hw % W;

    // 2-pass approach for stable softmax
    // 1) accumulate sum of exp
    float max_val = -1e30f;  
    for(int c = 0; c < C; c++){
        float val = input[b*C*H*W + c*H*W + h*W + w] + bias[c];
        if(val > max_val) max_val = val;
    }

    // sum of exp
    float sum_exp = 0.0f;
    for(int c = 0; c < C; c++){
        float val = input[b*C*H*W + c*H*W + h*W + w] + bias[c];
        float exp_val = expf(val - max_val); 
        sum_exp += exp_val;
    }
    sum_exp = (sum_exp <= 0.0f) ? 1e-30f : sum_exp;  // avoid div by zero

    // 2) write out normalized -> tanh -> scale
    for(int c = 0; c < C; c++){
        float val = input[b*C*H*W + c*H*W + h*W + w] + bias[c];
        float exp_val = expf(val - max_val);
        float soft_val = exp_val / sum_exp; 
        float tanh_val = tanhf(soft_val);
        float final_val = tanh_val * scale;
        output[b*C*H*W + c*H*W + h*W + w] = final_val;
    }
}

torch::Tensor add_bias_softmax_tanh_scale_cuda(torch::Tensor input,
                                               torch::Tensor bias,
                                               double scale) {
    TORCH_CHECK(input.is_cuda(), "Input must be a CUDA tensor");
    TORCH_CHECK(bias.is_cuda(), "Bias must be a CUDA tensor");
    TORCH_CHECK(input.dim() == 5, "Input must have 5 dims: (B, C, 1, H, W)");

    const int B = input.size(0);
    const int C = input.size(1);
    const int H = input.size(3);
    const int W = input.size(4);

    auto out = torch::zeros_like(input);

    const int total_bhw = B * H * W;
    const int block_size = 256;
    const int grid_size = (total_bhw + block_size - 1) / block_size;

    add_bias_softmax_tanh_scale_kernel<<<grid_size, block_size>>>(
        input.data_ptr<float>(),
        bias.data_ptr<float>(),
        out.data_ptr<float>(),
        B, C, H, W,
        static_cast<float>(scale)
    );
    return out;
}
""";

cpp_src = r"""
torch::Tensor mean_pool_3d_cuda(torch::Tensor input);
torch::Tensor add_bias_softmax_tanh_scale_cuda(torch::Tensor input, torch::Tensor bias, double scale);
""";

# Load the inline extension
my_kernels = load_inline(
    name="my_kernels_3d_optim",
    cpp_sources=cpp_src,
    cuda_sources=source,
    functions=[
        "mean_pool_3d_cuda",
        "add_bias_softmax_tanh_scale_cuda",
    ],
    verbose=False,
    extra_cflags=[],
    extra_ldflags=[],
)

# --------------------------------------------------------
# Optimized model definition using custom CUDA operators
# --------------------------------------------------------
class ModelNew(nn.Module):
    """
    Optimized version of the original Model using custom CUDA kernels:
      1. ConvTranspose3d (native PyTorch)
      2. Custom mean_pool_3d CUDA
      3. Custom bias+softmax+tanh+scale CUDA
    """
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, scaling_factor):
        super(ModelNew, self).__init__()
        self.conv_transpose = nn.ConvTranspose3d(
            in_channels, out_channels, kernel_size,
            stride=stride, padding=padding
        )
        self.bias = nn.Parameter(torch.randn(1, out_channels, 1, 1, 1))  # broadcastable
        self.scaling_factor = scaling_factor

    def forward(self, x):
        # 1) Transposed 3D convolution (PyTorch)
        x = self.conv_transpose(x)

        # 2) Mean pooling (across depth) via custom kernel
        x = my_kernels.mean_pool_3d_cuda(x)

        # 3) Combined bias addition, softmax (across channels), tanh, and scaling
        x = my_kernels.add_bias_softmax_tanh_scale_cuda(x, self.bias, self.scaling_factor)

        return x
```
